<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[都9102年了，还不会Docker？10分钟带你从入门操作到实战上手]]></title>
    <url>%2Fdocker-in-act%2F</url>
    <content type="text"><![CDATA[Docker简述Docker是一种OS虚拟化技术，是一个开源的应用容器引擎。它可以让开发者将应用打包到一个可移植的容器中，并且该容器可以运行在几乎所有linux系统中（Windows10目前也原生支持，Win10前需要内置虚拟机），正所谓“一次打包，到处运行”。 Docker容器的运行是完全的沙箱机制，相互之间不会有任何关联（除非自己串联集群）。网络、存储、进程等资源，不仅对于不同的容器是相互隔离，对于宿主机和容器直接也是隔离的，除非你手动映射暴露端口或者挂载存储卷。 很多人不理解，Docker和虚拟机到底有什么区别。从这两张结构图来看，Docker比虚拟机少了一层虚拟机操作系统，Docker的应用直接Docker引擎上运行。由于虚拟机需要一层操作系统，所以会导致虚拟机的体积非常大，通常在几G到十几G之间。并且通常一个虚拟机上，不只一个应用，因此对于整体的虚拟集群管理并不太友好，比较难做到灵活分配。 而一个Docker镜像的体积大约在几十M到几百M之间，一般一个镜像只打包一个应用，由多个镜像组成一个完整的项目，并且镜像易于复制，可以跨平台运行，这样可以使项目的部署管理有更好的灵活性。所以Docker无论从资源消耗上、管理上、使用上都在虚拟机之上，因此我们又有何理由不使用这样的容器化技术呢？ 对于容器化技术的学习，可谓是深如海。从基本的镜像、容器操作，到镜像的打包、容器的部署，再到企业生产级的容器集群管理技术（Docker官方的Swarm、Google的Kubernetes），如此多的内容，并不是所有人技术人员都能一朝学会。不过除了生产级别的集群管理技术有难度意外，其他内容从学习使用的角度来说，其实是非常简单的，况且K8s这种东西，对于普通开发来说也是很少能接触到。 说到这里，可能还有很多人觉得这个是公司层面、运维层面的操作，不是很了解Docker对于普通开发来说，意味着什么，对我们有什么好处？ 多办公环境，一键部署。假如你在公司一套开发环境，在家一套开发环境，当你公司的开发环境变更时，在家的环境就要跟着变，如果是使用Docker，将一些依赖型的应用，如Redis、ZK、Mysql等边缘服务都打包在docker里面。无论你在哪里改变了内容，只要在运行时更新下镜像，就可以按照最新的内容去执行了，不需要一个手动去安装，适配。 联调测试，无需依赖他人。当后端完成对外的接口后，将后端应用打包进docker，这样无论是前端、测试，在何地何时都可以自己把容器启动起来进行联调测试，而不需要自己手动一步步地搭建这个后端环境。 … 下面就来一步步讲解下，普通开发所需要的Docker知识。 概念介绍学习Docker首先要了解下几个基础概念： 宿主机，Host，运行Docker所在的物理机，是Docker运行的系统环境。 镜像，Image，相当于一个程序模板，通过这个模板可以生成很多个相似的容器。可以理解为Java中的类，它本身不具备执行运行的能力，是一个对象抽象的模板。每个镜像可以有多个版本，用tag来区分。镜像可以通过Dockerfile来构建。 容器，Container，Docker运行的最小单位对象。它是通过镜像实例化出来的一个可运行对象。容器的修改，可以提交反作用于镜像，更新这个容器的模板。 仓库，Repository，用于存储管理镜像的仓库，类似于git管理代码的仓库一样，可以管理多版本的镜像。 镜像、容器、仓库的关系如下：一句话总结就是，从仓库中拉取镜像，利用镜像生成容器。 基本操作了解完Docker的基本概念，我们开始来开始学习下入门操作。此处省略所有的Docker安装过程，自己去官网下载就行了，基本是傻瓜式安装。 拉取镜像通过docker pull ${image_uri}:${image_tag}命令，可以从远程仓库（默认是Docker Hub）中拉取所需要的镜像。 在Docker Hub的网站上可以搜索下自己需要的镜像以及版本。例如Ubuntu，上面提供了几个版本。我们拉一下16.04版本的ubuntu镜像。然后通过docker images命令，查看保存在本地镜像，发现多了一个ubuntu的镜像。 容器创建、启动、停止、登入有了镜像以后，就可以通过docker run -it ${image_id}创建启动一个容器了。 image_id是镜像的id，通过docker images能查看到，也可以是镜像名(REPOSITORY:TAG)。 -it可以让你在启动后，连上容器的终端。连上终端后，就可以在里面随意操作容器里面的内容了。 exit退出容器后，容器就会自动停止了。但是这个容器依然还存在，只是”关机“了。（可以通过ctrl+p,ctrl+q，退出容器登入，而不关闭容器） 通过docker ps -a可以看到我们的容器已经Exited了。 通过docker start ${container_id}，我们把这个容器再次启动。通过docker ps(加上-a包含显示未启动的容器)，可以看到容器的状态为UP。 同理，我们可以通过docker stop ${container_id}来停止容器， 在用docker start命令的时候，如果不加上-a参数，默认不会连接上容器的。不过我们可以在start后，通过docker attach ${container_id}来登入容器。 通过以上的基本操作，你基本可以利用docker当作一个虚拟机来使用了。如果想把容器和虚拟机的网络、存储打通，可以网上搜下了解下网络与卷挂载等容器设置。 更新镜像在上面的例子中，我们pull下来的仅仅是一个ubuntu的原始镜像，并没有过多的内容。下面我们在这个镜像的容器里面，安装一个jdk。这样我们的容器里面就有一个jdk了，但是如果我们再用这个ubuntu原始镜像再创建一个容器，它是不会用这个jdk的。所以我们就需要把这个容器的内容，提交到镜像当中。 通过docker commit ${container_id} ${repository}:${tag}，在本地将容器内容提交到镜像当中。然后就可以拥有一个带jdk的ubuntu镜像了。后面我们就可以利用这个镜像，生成带jdk的容器了。 以上的更新仅限于在本地的镜像，如果想把容器推送到云端就需要用docker push命令。前提是你已经登录了仓库拥有权限。 镜像仓库上面提到，默认情况下，仓库是用Docker Hub。我们pull 和push都是在Docker hub上操作，但是如果镜像是内部私有使用的话，没有必要去使用Docker Hub，一个是网络慢，另一个是私有安全性问题。 针对以上问题，有两种解决方法，一个是自己搭建私有服务，另一个是用云服务的镜像管理平台（如阿里云的“容器镜像服务”）。前者对于一般开发者来说并没有必要，而且还要搞认证的，比较麻烦，这里不细说。下面介绍下如何用阿里云服务作为自己的私有仓库。 先在阿里云上创建一个镜像仓库，获得一个仓库地址，如registry.cn-shenzhen.aliyuncs.com/zackku/jdk。这里一个仓库地址，对应一种镜像（tag不同）。 利用docker login，先对阿里云的服务进行登录。 然后对上面的jdk镜像打tag（其实也是改仓库源的过程） 最后把镜像推送到阿里云就行了。 推送后，就能在阿里云的仓库上看到这个镜像。 通过搭建私有仓库，我们就可以完全抛开宿主机的环境，构建好一个镜像，就可以到处运行了。 Dockerfile构建镜像从上面介绍，我们已经了解到，如何从拉取一个镜像、修改容器内容、提交镜像去构建一个我们所需要的镜像。但通过这些操作去构建一个镜像，一个是太繁琐，另一个问题是不清晰，没办法直观的了解镜像的构成。 Dockerfile就可以很好的解决该问题。它可以通过编写一个构建过程，来一站式构建镜像。下面同样以ubuntu为基础镜像，安装jdk构建一个新镜像为例，看看Dockerfile是怎么写的。 然后执行docker build -t registry.cn-shenzhen.aliyuncs.com/zackku/jdk2:1.0 .就能把镜像构建出来了。 Dockerfile高级技巧上面是Dockerfile的基本使用，但实际情况下我们并不像（或者说不仅是）上面描述那样去构建镜像。下面介绍两个常用的使用原则。 分层构建。其实Docker的镜像是分层结构的，看回之前推送到远端仓库的例子。红框里面就是镜像一层层的提交，如果这层已经本地构建过了，下次不需要构建了，同理如果远端已经有这层了，也不需要推送这层。而且这种分层是可以在不同镜像间共享的，例如不同的Java项目都是依赖于JDK的运行环境，那么它们就可以共用JDK这层镜像内容。 所以，基于这样的特性，我们就应该要分层去构建镜像，抽象镜像共同点。具体操作的话，我们大致可以去分两次构建镜像，先构建一个base镜像，用于不同镜像的底层，例如Java项目的所有基础运行环境，然后再通过base镜像，构建develop表层的应用镜像。相当于把应用程序打包丢到develop层里面。并且这层要告诉Docker是怎么运行程序的。 尽量构建小的base层。镜像的体积也是在使用Docker的时候要考虑的一个重要因素，因为如果镜像的体积过大，在更新镜像，拉取镜像的时候效率会低。尤其在刚刚所说的base层里面，如果base层做得太大太臃肿，里面程序过多，不仅仅体积大，还会让CPU、网络等资源消耗过大。其实我们在用Docker的时候，一般是一个容器只包含一个程序项目，关于这个程序的监控、健康等内容，在容器外通过k8s等集群管理去做，所以容器本身只需要保证自己的程序能够运行起来就行了。 至于上面我用ubuntu作为基础的操作系统是比较多余的，这里推荐只用apline操作系统作为程序的最底层镜像，它是一款轻型的Linux发行版，系统体积与运行时的资源消耗都相当低，十分适合用于Docker容器。基于apline的操作系统，我们在上面添加自己所需要的环境，例如安装一个Tomcat、JDK等，从而构建一个base的镜像。 上所说的base镜像，其实不太需要自己的写一层Dockfile，docker官方就直接提供了各种语言、环境的基础镜像，在github的docker-library里面。如果再有自己的团队的运行环境的要求，可以在这个Dockerfile基础上去添加修改即可，或者再抽象多一层。 至于Dockfile怎么写，语法是什么，网上有大把详细的说明，由于篇幅问题，不在这里展开。 docker-compose启动集群前面已经介绍完一个单独的容器是如何构建与启动的了，但我们的项目往往不是只有一个容器的，把所有程序打包在一个容器不是正确的做法。所以我们怎么去管理启动这么多的容器，是一个必修的课题。在企业级的层面，有K8S，Swarm这种容器编排的管理工具，但稍微比较复杂，个人使用的话也没有太大必要。 这里推荐用Docker官方的docker-compose，它可以把所有的容器编排方式写在一个文件里，然后通过docker-compose up命令，就可以把一套的容器按照你的编排全部启动起来。 在这个例子的services包含每个容器的配置，其中的redis、mongodb用的是默认的镜像、默认的配置，myproject是我们自己的项目。通过这样的编排，我们就能让我们的项目连上redis和mongodb。最后通过docker-compose up就会自动拉取镜像，按照编排跑起来了。 具体的语法也不赘述，关键就是容器的卷挂载，网络的配置，端口的暴露，容器的依赖关系。如果把这套东西用起来，慢慢自然就会了解，重要的是动手去做一遍，尝试一下。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>都9102年了，还不会docker？</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“过时”的SpringMVC我们到底在用什么？深入分析DispatchServlet源码]]></title>
    <url>%2Fspring-mvc%2F</url>
    <content type="text"><![CDATA[之前已经分析过了Spring的IOC(《零基础带你看Spring源码——IOC控制反转》)与AOP(《从源码入手，一文带你读懂Spring AOP面向切面编程》)的源码，本次就来分析下SpringMVC。本文先简述下目前SpringMVC的使用情况，然后通过Demo的简单让大家有一个初步的使用印象，然后带着印象去看其中执行的分发源码。 到底什么是Spring MVC，我们还在用吗？Spring MVC，官方名字其实是Spring Web MVC，Maven上的包名也是spring-webmvc。从Spring诞生以来，它就是一款基于Servlet Api的web架构。值得一提的是，在Spring5的时候，出了一款新的Web架构，Flux，是基于事件驱动模型（类似nodejs）做的。以后会写一篇来专门介绍一下Flux，敬请关注。 MVC，可以说是“上个世纪”最流行的前后端交互模型。它包含Model（业务模型）、View（用户视图）、Controller（控制器），把各部分分开组织，对代码抽象与隔离的处理可谓是代码设计的典范。 不过自从15年开始，随着各种前端框架的崛起，使得前端后端的关系发生进一步的演变，从MVC架构演变成前后端分离的REST架构了。以前MVC架构每次请求都需要经过控制器-&gt;模型-&gt;视图的流程，演变成前端请求后端接口，返回JSON的这样一种REST架构。问题来了，我们到底还在用SpringMVC吗？答案是，不全用。前后端做了代码以及部署的分离，也就是说后端并不感知前端的存在，所以对于后端而言，View（用户视图）也就无从可谈了。Model（业务模型）发送性质上的改变，以前是一个前端所需要的Model，给页面读取，现在是一个JSON格式给到前端，由前端自由处理。 而作为Web框架的核心，Controller（控制器）则是依然留存的。所以现在大家用SpringMVC用的更多是Controller这一层。当然SpringMVC还有其他组件，包括filter、Http Caching、Web Security等等。本文只是着重MVC架构中的Controller的功能，而Controller的核心组件则是DispatcherServlet。所以后面我们将通过Demo，来逐步深入了解下，DispatcherSevlet如何做到对请求控制分发的。 传统SpringMVC启动简述在传统的SpringMVC中，需要配置web.xml和applicationContext.xml。前者是负责配置项目初始化的配置，如servlet、welcome页面等，是JavaEE的规范。后者是初始化Spring Context的配置，主要是Bean的配置。 前文说到，SpringMVC是基于Servlet的架构，而DispatcherServlet则是SpringMVC拦截处理所有请求的Servlet，所以web.xml需要配置DispatcherServlet。其他的还有contextLoaderListener，负责加载除DispatcherServlet外的所有context内容，另外还需要通过contextConfigLoader指定Spring的配置文件（如applicationContext.xml）。 那么在项目启动的时候，加载web.xml首先会执行contextLoaderListener，让它初始化好Spring的Application context。后面有HTTP请求进来，则会落到DispatcherServlet上，让它去做处理分发。 SpringBoot Web Demo搭建自从Spring配置注解和SpringBoot诞生以来，越来越少人去写web.xml和applicationContext.xml配置文件了。但为了方便直接了解Dispatcher的原理，Demo直接用SpringBoot的starter一键式搭建。 直接添加web的starter依赖12345 &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;version&gt;2.0.4.RELEASE&lt;/version&gt;&lt;/dependency&gt; 看下这个starter包含什么内容绿框是springMVC的依赖，红框是Spring自动配置的依赖，蓝框则是内嵌tomcat的依赖。里面Spring的版本是5.0.8 RELEASE的。 SpringBoot启动类 测试controller 启动项目后，在浏览器里面输入http://localhost:8080/hello?name=Zack。结果返回Hello Zack。 以上就是我们现在利用SpringMVC的基本内容，下面我们来看下SpringMVC如何利用DispatcherServlet做拦截分发的。 DispatcherServlet源码分析当一个请求进来的时候，会先执行各种filter，过滤掉最终需要的请求，然后会落到DispatcherServlet中的doService()方法。该方法是预先设置一些特殊请求参数，然后再转发给doDispatch()做真正的处理转发。 看一下doDispatch()的注释说明该方法的作用就是执行实际分发到的handler。 Handler通过HandlerMapping的优先级获取。HandlerAdapter通过查询DispatcherServlet已装载的HandlerAdapter，并且支持该Handler而获取的。 所有的HTTP请求都是doDispatch()去处理的。具体是落到哪个方法去处理业务逻辑，取决于HandlerAdapters或者handlers。 从注释可知，整个的分发逻辑核心，就在于HandlerAdapter和Handler。那这两到底是什么东西？ 官网上的说明HandlerAdapter协助DispatcherServlet去调用对应的handler，忽略具体handler是怎么调用的。例如调用注解形式的controller需要处理注解，xml配置形式的要解析配置文件。这个适配器就是为了帮助DispatcherServlet屏蔽掉处理具体的细节。 至于Handler没有清晰解释，但我们debug源码可以发现，Handler其实就是实际分配到具体需要去处理的方法(对比下图红框和上面Demo的controller)。 回到doDispatch()这个方法的源码上，看到getHandler()、getHandlerAdapter()就是获取Handler和HandlerAdapter所在。 getHandler()看下getHandler()源码整个方法就那么几行，不过需要注意有两个点。一个是该方法是返回HandlerExecutionChain类型，而不是一个Handler。HandlerExecutionChain其实就是Handler的一层封装，还包含Handler对应的interceptor拦截器，用于执行Handler的一些前置和后置的操作。 另外一个点，HandlerExecutionChain是按顺序遍历handlerMappings拿出来的。那HandlerMapping又是什么呢？从官网说明可知，它是一个请求和handler（实际是HandlerExecutionChain）的关联Map，通俗的说就是路由与处理逻辑的关联。它主要有两个实现，一个是RequestMappingHandlerMapping（支持注解形式方法），另一个是SimpleUrlHandlerMapping（维护显示注册的URI资源）。 由此可推测，在Spring启动的时候，就会去扫描注解、注册的静态资源，从而初始化这个handlerMappings。具体逻辑就在DispatcherServlet中的initHandlerMappings方法内。初始化的方法内，主要有三步： 从Spring的ApplicationContext中取出HandlerMapping的Bean 然后对上面取出来的Bean做优先级排序，主要对是@Order注解的排序 如果上面取不出Bean，则用默认策略。 对于第三点的默认策略，可以找到DispatcherServlet.properties这个文件，里面配置了一些默认HandlerMapping、HandlerAdapter等相关类。 在初始化handlerMappings后，如果有请求进来，后面的request就用请求的路由与HandlerMapping对比，最后找出Handler（HandlerExecutionChain）。 getHandlerAdapter()在取出实际处理的Handler后，就需要用它找出support它的适配器（HandlerAdapter）。按照前面对HandlerAdapter的描述，对于Demo而言，support这个Handler必定是RequestMappingHandlerAdapter。 这个逻辑也非常简单，同样是遍历已初始化的handlerAdapters（初始化的过程类似handlerMappings），然后对于具体每个handlerAdapter，调用其support()方法，看是否支持。 supports()方法也很简单，就用instanceof判断handler是否Adapter自己支持的类。 HandlerAdapter.handle()在获取完Handler和HandlerAdapter后，就可以执行HandlerAdapter中的handle方法，其实际只是调用Handler的方法。 我们按Demo例子，看下HttpRequestHandlerAdapter的handle()方法实现。这个方法里面就是用HttpServlet的Request和Reponse去调用我们自己写的controller里面的方法。需要注意的是，这个方法返回的是ModelAndView，但我们目前基于Rest架构是已经不用的了，所以方法返回null回去了。 Handler的前置后置处理前面提到Handler是被封装在HandlerExecutionChain里面的，其中还包含一些前置后置的拦截器。所以在执行HandlerAdapter.handle()前后会有对HandlerExecutionChain的调用，执行interceptor对前后置处理的方法 具体里面的实现就是执行interceptor的preHandle()和postHandle()方法。 回过头来想下，这里的前后置处理会包括什么呢？在HandlerInterceptor注解上有说明三个实现类，分别是UserRoleAuthorizationInterceptor（检查用户权限）、LocaleChangeInterceptor（修改本地时间）、ThemeChangeInterceptor（修改当前主题）。可以看出HandlerInterceptor基本都是对请求的一些预处理和结果封装。 总结以上就是SpringMVC中DispatcherServlet的基本过程。下面来总结下以上内容： 前后端的架构演变导致SpringMVC的使用发生改变，更多着重在“C”上了。 “C”的核心在DispatcherServlet的doDispatcher()方法中。 利用request的路由，对比从已初始化的handlerMappings和handlerAdapters中获取handler和handlerAdapter。 handler是封装在HandlerExecutionChain中，其中还包括handler的前后置拦截器。 最后利用适配器模式，调用HandlerAdapter.handle()方法去执行handler具体处理的业务逻辑。 在执行具体业务逻辑前后会执行封装在HandlerExecutionChain里面的拦截器。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Spring分析学习</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>MVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进阶的Redis之哈希分片原理与集群实战]]></title>
    <url>%2Fredis-cluster%2F</url>
    <content type="text"><![CDATA[前面介绍了《进阶的Redis之数据持久化RDB与AOF》和《进阶的Redis之Sentinel原理及实战》，这次来了解下Redis的集群功能，以及其中哈希分片原理。 集群分片模式如果Redis只用复制功能做主从，那么当数据量巨大的情况下，单机情况下可能已经承受不下一份数据，更不用说是主从都要各自保存一份完整的数据。在这种情况下，数据分片是一个非常好的解决办法。 Redis的Cluster正是用于解决该问题。它主要提供两个功能： 自动对数据分片，落到各个节点上 即使集群部分节点失效或者连接不上，依然可以继续处理命令 对于第二点，它的功能有点类似于Sentienl的故障转移（可以了解下之前Sentinel的文章），在这里不细说。下面详细了解下Redis的槽位分片原理，在此之前，先了解下分布式简单哈希算法和一致性哈希算法，以帮助理解槽位的作用。 简单哈希算法假设有三台机，数据落在哪台机的算法为1c = Hash(key) % 3 例如key A的哈希值为4，4%3=1，则落在第二台机。Key ABC哈希值为11，11%3=2，则落在第三台机上。 利用这样的算法，假设现在数据量太大了，需要增加一台机器。A原本落在第二台上，现在根据算法4%4=0，落到了第一台机器上了，但是第一台机器上根本没有A的值。这样的算法会导致增加机器或减少机器的时候，引起大量的缓存穿透，造成雪崩。 一致性哈希算法在1997年，麻省理工学院的Karger等人提出了一致性哈希算法，为的就是解决分布式缓存的问题。 在一致性哈希算法中，整个哈希空间是一个虚拟圆环 假设有四个节点Node A、B、C、D，经过ip地址的哈希计算，它们的位置如下 有4个存储对象Object A、B、C、D，经过对Key的哈希计算后，它们的位置如下对于各个Object，它所真正的存储位置是按顺时针找到的第一个存储节点。例如Object A顺时针找到的第一个节点是Node A，所以Node A负责存储Object A，Object B存储在Node B。 一致性哈希算法大概如此，那么它的容错性和扩展性如何呢？ 假设Node C节点挂掉了，Object C的存储丢失，那么它顺时针找到的最新节点是Node D。也就是说Node C挂掉了，受影响仅仅包括Node B到Node C区间的数据，并且这些数据会转移到Node D进行存储。 同理，假设现在数据量大了，需要增加一台节点Node X。Node X的位置在Node B到Node C直接，那么受到影响的仅仅是Node B到Node X间的数据，它们要重新落到Node X上。 所以一致性哈希算法对于容错性和扩展性有非常好的支持。但一致性哈希算法也有一个严重的问题，就是数据倾斜。 如果在分片的集群中，节点太少，并且分布不均，一致性哈希算法就会出现部分节点数据太多，部分节点数据太少。也就是说无法控制节点存储数据的分配。如下图，大部分数据都在A上了，B的数据比较少。 哈希槽Redis集群（Cluster）并没有选用上面一致性哈希，而是采用了哈希槽（SLOT）的这种概念。主要的原因就是上面所说的，一致性哈希算法对于数据分布、节点位置的控制并不是很友好。 首先哈希槽其实是两个概念，第一个是哈希算法。Redis Cluster的hash算法不是简单的hash()，而是crc16算法，一种校验算法。 另外一个就是槽位的概念，空间分配的规则。其实哈希槽的本质和一致性哈希算法非常相似，不同点就是对于哈希空间的定义。一致性哈希的空间是一个圆环，节点分布是基于圆环的，无法很好的控制数据分布。而Redis Cluster的槽位空间是自定义分配的，类似于Windows盘分区的概念。这种分区是可以自定义大小，自定义位置的。 Redis Cluster包含了16384个哈希槽，每个Key通过计算后都会落在具体一个槽位上，而这个槽位是属于哪个存储节点的，则由用户自己定义分配。例如机器硬盘小的，可以分配少一点槽位，硬盘大的可以分配多一点。如果节点硬盘都差不多则可以平均分配。所以哈希槽这种概念很好地解决了一致性哈希的弊端。 另外在容错性和扩展性上，表象与一致性哈希一样，都是对受影响的数据进行转移。而哈希槽本质上是对槽位的转移，把故障节点负责的槽位转移到其他正常的节点上。扩展节点也是一样，把其他节点上的槽位转移到新的节点上。 但一定要注意的是，对于槽位的转移和分派，Redis集群是不会自动进行的，而是需要人工配置的。所以Redis集群的高可用是依赖于节点的主从复制与主从间的自动故障转移。 集群搭建下面以最简单的例子，抛开高可用主从复制级转移的内容，来重点介绍下Redis集群是如何搭建，槽位是如何分配的，以加深对Redis集群原理及概念的理解。 redis.conf配置先找到redis.conf，启用cluster功能。 cluster-enabled yes默认是关闭的，要启用cluster，让redis成为集群的一部分，需要手动打开才行。 然后配置cluster的配置文件每一个cluster节点都有一个cluster的配置文件，这个文件主要用于记录节点信息，用程序自动生成和管理，不需要人工干预。唯一要注意的是，如果在同一台机器上运行多个节点，需要修改这个配置为不同的名字。 本次为了方便搭建，所有Redis实例都在同一台机器上，所以修改不同的cluster config名字后，复制三份redis.conf配置，以用于启动三个集群实例（cluster至少要三个主节点才能进行）。 集群关联123&gt; redis-server /usr/local/etc/redis/redis-6379.conf --port 6379 &amp;&gt; redis-server /usr/local/etc/redis/redis-6380.conf --port 6380 &amp;&gt; redis-server /usr/local/etc/redis/redis-6381.conf --port 6381 &amp; &amp;符号的作用是让命令在后台执行，但程序执行的log依然会打印在console中。也可以通过配置redis.conf中deamonize yes，让Redis在后台运行。 连上6379的Redis实例，然后通过cluster nodes查看集群范围。连上其他实例也是一样，目前6379、6380、6381在各自的集群中，且集群只有它们自己一个。 在6379上，通过cluster meet命令，与6380、6381建立链接。12127.0.0.1:6379&gt; cluster meet 127.0.0.1 6380127.0.0.1:6379&gt; cluster meet 127.0.0.1 6381 可以看到集群中已经包含了6379、6380、6381三个节点了。登录其他节点查看也是一样的结果。即使6380与6381之间没有直接手动关联，但在集群中，节点一旦发现有未关联的节点，会自动与之握手关联。 槽位分配通过cluster info命令查看集群的状态state的状态是fail的，还没启用。看下官方的说明只有state为ok，节点才能接受请求。如果只要有一个槽位（slot）没有分配，那么这个状态就是fail。而一共需要分配16384槽位才能让集群正常工作。 接下来给6379分配0~5000的槽位，给6380分配5001~10000的槽位，给6381分配10001~16383的槽位。123&gt; redis-cli -c -p 6379 cluster addslots &#123;0..5000&#125;&gt; redis-cli -c -p 6380 cluster addslots &#123;5001..10000&#125;&gt; redis-cli -c -p 6381 cluster addslots &#123;10001..16383&#125; 再看看cluster infostate已经为ok，16384个槽位都已经分配好了。现在集群已经可以正常工作了。 效果测试随便登上一个实例，记得加上参数-c，启用集群模式的客户端，否则无法正常运行。1redis-cli -c -p 6380 尝试下set、get操作可以看到，Redis集群会计算key落在哪个卡槽，然后会把命令转发到负责该卡槽的节点上执行。 利用cluster keyslot命令计算出key是在哪个槽位上，从而得出会跳转到哪个节点上执行。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Redis进阶</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进阶的Redis之数据持久化RDB与AOF]]></title>
    <url>%2Fredis-rdb-aof%2F</url>
    <content type="text"><![CDATA[大家都知道，Redis之所以性能好，读写快，是因为Redis是一个内存数据库，它的操作都几乎基于内存。但是内存型数据库有一个很大的弊端，就是当数据库进程崩溃或系统重启的时候，如果内存数据不保存的话，里面的数据就会丢失不见了。这样的数据库并不是一个可靠的数据库。 所以数据的持久化是内存型数据库的重中之重。它不仅提供数据保存硬盘的功能，还可以借此用硬盘容量扩展数据存储空间，使得Redis的可以存储超过机器本身内存大小的数据。 Redis对于数据持久化提供了两种持久化的方案，RDB与AOF。它们的原理和使用场景都大不相同，下面我们来详细地了解下。 RDB——数据快照（Snapshot）RDB，提供一个某个时间点的数据的Snapshot，保存在RDB文件中。它可以通过SAVE/BGSAVE命令手动执行，把数据Snapshot写到RDB文件，也可以通过配置，定时执行。 Redis也可以通过加载RDB文件，把数据从磁盘加载读取到Redis中。 RDB文件创建连上Redis，设值一些值，然后执行SAVE命令。 然后可以查看下redis.conf的持久化工作目录。进入目录可以看到保存了一个dump.rdb文件。该文件是一个二进制文件，无法直接正常打开。 至于SAVE/BGSAVE的区别，就是前置是阻塞执行，此时服务不会接受请求，后者是Fork一个子进程出来，由该进程去执行保存RDB文件的操作，不影响用户请求。 P.S. Redis是单进程的，所以BGSAVE只能Fork一个子进程，而不是创建一个线程处理。 以上是手动执行的过程。但在生产我们很少会手动登上服务去执行操作，所以更多的时候是依赖Redis的配置，定时保存RDB文件。 打开redis.conf配置文件，找到SNAPSHOTTING的配置，Save Point的设置。图中的配置意思是，当至少有一个key变更时，900秒后会执行一次SAVE。其他配置同理，有10次变更，300秒后保存一次….. 在Redis中，这个自动保存RDB的功能是默认开启的。 RDB文件加载先kill掉Redis进程，再重新启动Redis Server，会发现日志会有这样的一行， 并且Redis中，依然有之前设置的三个值。说明Redis在启动的时候，会加载数据初始化。 不过，这里加载的初始化数据不一定是RDB的。如果Redis开启了AOF，会优先从AOF初始化数据，否则才会加载RDB的数据。 RDB优缺点优点： RDB是某一时间点的快照，是一个紧凑的单文件，更多用于数据备份。可以按每小时或每日来备份，方便从不同的版本恢复数据。 单文件容易传输到远程服务做故障恢复。 RDB可以Fork子进程进行持久化，使Redis可以更好地处理用户请求 在大量数据的情况下，RDB相比较于AOF会更快的加载。 缺点： 如果Redis不及时保存RDB文件，会造成数据的丢失。例如系统突然断电，但未来得及保存数据。即使你设置更多的Save point，也无法保证100%的数据不丢失。 RDB经常需要fork子进程去执行，但如果再大量数据的情况下，这个fork操作会非常耗CPU资源的。对比AOF虽然也是fork，但是它的数据保存处理是可以控制的，不需要全量保存。 AOF——日志追加（Append-Only）Redis的另外一种持久化方案就是AOF，Append Only File。AOF相当于一个操作的日志记录，每次对于数据的变更都会记录追加到AOF日志。当服务启动的时候就会读这些操作日志，重新执行一次操作，从而恢复原始数据。 AOF启用AOF默认是关闭的。打开redis.conf配置文件，找到appendonly no改成appendonly yes。AOF和RDB是可以共存的，只要保存的文件名不冲突。 AOF fsync同步规则配置文件往下拉，看到fsync的配置。fsync()是一个系统调用函数，告诉操作系统把数据写到硬盘上，而不是缓存更多数据才写到硬盘。这样的调用可以及时保存数据到硬盘上。 Redis提供了三种fsync的调用方式 appendfsync always，每次操作记录都同步到硬盘上，最低效，最安全。 appendfsync everysec，每秒执行一次把操作记录同步到硬盘上。默认选项。 appendfsync no，不执行fysnc调用，让操作系统自动操作把缓存数据写到硬盘上，不可靠，但最快。 AOF文件格式解析开启AOF后，会再工作目录看到appendonly.aof文件。在客户端上执行一些命令后，打开AOF文件，可以观察到有对应的操作的记录日志。文件解析说明： *，表示命令的参数个数，例如set a 1是三个参数，所以是*3 $，表示参数的字节数，例如set这个参数是三字节，所以是$3，key值a是一个字节，所以是$1 无符号，表示是参数的数据，例如set,a,1就是具体的数据 日志重写AOF虽然比RDB更可靠，但缺点也是比较明显的，就是每次写操作都要把操作日志写到文件上，这样会导致文件非常冗余。 假若你要自增一个计数器100次，如果不重写，AOF文件就就会有这100次的自增记录，如INCR a。如果执行了日志重写，那么文件只会保留set a 100而不是100条INCR a。这样拥有相同的结果，但可以大大减少AOF的文件大小，并且可以让AOF载入的时候提升载入的效率。 看回redis.conf配置，有两项控制rewrite的选项。 auto-aof-rewrite-percentage 100，当文件增长100%（一倍）时候，自动重写。 auto-aof-rewrite-min-size 64mb，日志重写最小文件大小，如果小于该大小，不会自动重写。 来实验一下重写的结果，我们先设定一个a值，然后自增多次，查看AOF文件内容。里面有很多INCR的语句记录 然后我们手动执行下BGREWRITEOF，执行日志重写。可以看到，多个incr语句，变成了一个set a 6语句，减少了5个incr a语句的操作日志。 AOF优缺点优点： AOF可以设置 完全不同步、每秒同步、每次操作同，默认是每秒同步。因为AOF是操作指令的追加，所以可以频繁的大量的同步。 AOF文件是一个值追加日志的文件，即使服务宕机为写入完整的命令，也可以通过redis-check-aof工具修复这些问题。 如果AOF文件过大，Redis会在后台自动地重写AOF文件。重写后会使AOF文件压缩到最小所需的指令集。 AOF文件是有序保存数据库的所有写入操作，易读，易分析。即使如果不小心误操作数据库，也很容易找出错误指令，恢复到某个数据节点。例如不小心FLUSHALL，可以非常容易恢复到执行命令之前。 缺点 相同数据量下，AOF的文件通常体积会比RDB大。因为AOF是存指令的，而RDB是所有指令的结果快照。但AOF在日志重写后会压缩一些空间。 在大量写入和载入的时候，AOF的效率会比RDB低。因为大量写入，AOF会执行更多的保存命令，载入的时候也需要大量的重执行命令来得到最后的结果。RDB对此更有优势。 如何选择以上已经基本了解过RDB和AOF的使用、基本原理以及对应的优缺点。那么在实际当中，我们到底怎么去选择用哪种持久化方式呢？ 一般来说，不考虑硬盘大小，最安全的做法是RDB与AOF同时使用，即使AOF损坏无法修复，还可以用RDB来恢复数据。 如果Redis的数据在你的服务中并不是必要的数据，例如只是当简单的缓存，没有缓存也不会造成缓存雪崩。说明数据的安全可靠性并不是首要考虑范围内，那么单独只使用RDB就可以了。 不推荐单独使用AOF，因为AOF对于数据的恢复载入来说，比RDB慢。并且Redis官方也说明了，AOF有一个罕见的bug。出了问题无法很好的解决。所以使用AOF的时候，最好还是有RDB作为数据备份。 根据官方的意愿描述，在未来可能会有一种RDB与AOF相结合的持久化模型。到时Redis持久化就不再如此麻烦费劲了，我们拭目以待吧。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Redis进阶</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进阶的Redis之Sentinel原理及实战]]></title>
    <url>%2Fredis-sentinel%2F</url>
    <content type="text"><![CDATA[Redis作为一款高效的内存数据库，可作用于方方面面，相信如今项目的开发都离不开它。大家可能都知道Redis是高可用的，但很少知道具体高可用是利用什么去实现的。 抛两个问题： 只部署一个Redis实例，如果这个实例挂了就无法读写数据了，那怎么做实例备份？ 部署了两个Redis，一主一从做复制，从只读，如果主挂了，那这个服务还怎么正常对外服务？ 显然，要做到高可用，首先要有足够多的Redis实例（最好三台以上），一主多从。然后再主挂了的时候，要有机制让其他实例替代主的位置。 哨兵服务Sentinel，就是这套题的答案，它是一个检查redis服务下线并补偿的服务。下面我们来完整了解下Sentinel的作用和工作原理，最后实验下它的效果。 Sentinel简介直接看下最新版的官方的介绍大致意思是，Sentinel为Redis提供高可用。利用Sentinel，在无人干预的情况下，可用让Redis服务抵御一定程度的故障。 宏观层面，Sentinel拥有以下几个功能： 监控（Monitoring），Sentinel可用持续不断地检查主从实例是否如期运行。 通知（Notification），当某个被监控的Redis实例出问题的时候，可以通过API接口向系统管理员和其他应用服务发通知。 自动故障转移（Automatic failover），当主出现故障时，Sentinel会自动启动故障转移流程，把其中一个从库提升为主库，然后其他从库重新认新主。集群也会返回新的地址给客户端。 配置提供（Configuration provider），Sentinel可以作为服务注册中心，让客户端直接连接请求Sentinel去获取主库的地址。如果出现自动故障转移，Sentinel也会提供新的主库地址。 P.S. 文中所描述的库和服务器都是指Redis的server实例。 高可用工作原理下面我们以Sentinel的故障转移为核心，来看看它具体是怎么实现Sentinel的。 启动并初始化Sentinel执行 redis-sentinel /usr/local/etc/redis-sentinel.confSentinel是Redis的特殊模式，执行redis-server /usr/local/etc/redis-sentinel.conf --sentinel命令也是完全一样的。其实在启动Sentinel的时候，redis只是把运行代码切换到sentinel模式。 初始化master属性配置文件中是需要指定了监控的主库，然后再初始化阶段把Sentinel实例中的master属性初始化。 Sentinel向主库建立网络连接在master属性初始化后，Sentinel首先就需要和该主库建立网络连接。Sentinel会对每个被监视的主库建立两个链接。一个命令链接，用于给主库发送操作命令的。另一个是订阅链接，订阅主库的__sentinel__:hello频道，主要用于发现其他Sentinel的存在（后面会说到）。 获取主库信息Sentinel默认以十秒一次，通过命令链接，给主库发送INFO命令，来获取主库的当前信息。信息包括该主库的运行id以及该主库下所有从库的ip端口。 根据主库信息，获取从库信息如果Sentinel在获取主库信息时候，发现有新未链接的从库，会与该从库同样建立两条链接。建立链接后，Sentinel同样会给从库通过命令链接发送INFO命令，从而获取该从库的服务器信息。 向主和从发送服务器消息Sentinel默认以两秒一次，通过命令链接，对__sentinel__:hello频道发送Sentinel自身信息与被监控的主库信息。该频道是之前Sentinel与所有主从库都建立的频道链接。所以被监视的主从库都会收到发送消息的Sentinel信息与主库信息。 接收主从频道消息，找出其他Sentinel因为Sentinel订阅了__sentinel__:hello的消息，所以在之前向主从库发送消息的时候，同时会通过订阅链接收到订阅的内容。这个作用在于，部署多个Sentinel时，其他Sentinel就会知道发送消息的那个Sentinel的存在。 与其他Sentinel创建连接当Sentinel发现有其他Sentinel存在的时候，就会与其他Sentinel建立一条命令链接，用于在后续情况发送命令。但Sentinel之间不会建立订阅链接，因为订阅链接是用于发现其他Sentinel的存在的。 主观下线Sentinel默认以一秒一次，通过命令链接，对所有服务器（包括主从、其他Sentinel）发送PING命令来检测服务是否在线。如果一个服务在参数down-after-millisecond内连续返回无效回复，那么Sentinel会对该服务标记为主观下线状态。所谓的主观下线，就是Sentinel自己认为该服务下线了。 主库客观下线当Sentinel将一个主库主观下线后，为了判断是否其他Sentinel都认为该主库下线了，会通过is-master-down-by-addr命令询问。当认为下线的Sentinel数据大于配置中的quorum数时，Sentinel会标记该主库为客观下线。 选举Sentinel leader当Sentinel将主库客观下线后，会与其他Sentinel进行协商，选举出一个leader来执行后续的Failover（故障转移）。 选出新主库。leader按从库优先级配置、复制偏移量、运行id来选举出新的主库。leader会向挑选出来的从库发送SLAVEOF no one命令，将从库提升为master角色，成为主库。 让从库复制新主库在选出新主库后，就是让原来的从库去复制新主库。leader Sentinel通过给剩余旧从库发送SLAVEOF命令即可。 旧主库变为新主库的从库当原来故障的主库再次上线的时候，Sentinel会发送SLAVEOF命令，让其成为新主库的从库。实验实战 启动三个Redis实例 123redis-server --port 6379 &amp;redis-server --port 6380 &amp;redis-server --port 6381 &amp; 注意最好各个实例的priority不同，用于故障转移选主。否则可能会找不到新主。 把80与81的实例作为79的从服务器在80与81上执行 1SLAVEOF 127.0.0.1 6379 启动sentinel 1redis-sentinel /usr/local/etc/redis-sentinel.conf 其中的Sentinel配置包含 1sentinel monitor mymaster 127.0.0.1 6379 1 可以观察到sentinel已经添加了一主两从的监控。 把主库的实例关掉 1127.0.0.1:6379&gt; SHUTDOWN 故障转移redis-cli连上sentinel可以看到，当sentinel检测到主库下线，后面根据一些列操作把81替换为主库，其他库为81的从库，并且79是下线的新从库。 参考文献： 《Redis设计与实现》——黄健宏 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Redis进阶</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈分布式CAP定理]]></title>
    <url>%2Fcap%2F</url>
    <content type="text"><![CDATA[互联网发展到现在，由于数据量大、操作并发高等问题，大部分网站项目都采用分布式的架构。而分布式系统最大的特点数据分散，在不同网络节点在某些时刻（数据未同步完，数据丢失），数据会不一致。 在2000年，Eric Brewer教授在PODC的研讨会上提出了一个猜想：一致性、可用性和分区容错性三者无法在分布式系统中被同时满足，并且最多只能满足其中两个！ 在2002年，Lynch证明其猜想，上升为定理。被这就是大家所认知的CAP定理。 CAP是所有分布式数据库的设计标准。例如Zookeeper、Redis、HBase等的设计都是基于CAP理论的。 CAP定义所谓的CAP就是分布式系统的三个特性： Consistency，一致性。所有分布式节点的数据是否一致。 Availability，可用性。在部分节点有问题的情况（数据不一致、节点故障）下，是否能继续响应服务（可用）。 Partition tolerance，分区容错性。允许在节点（分区）数据不一致的情况。 深入理解有A、B、C三个分布式数据库。 当A、B、C的数据是完全相同，那么就符合定理中的Consistency（一致性）。 假如A的数据与B的数据不相同，但是整体的服务（包含A、B、C的整体）没有宕机，依然可以对外系统服务，那么就符合定理中的Availability（可用性）。 分布式数据库是没有办法百分百时刻保持各个节点数据一致的。假设一个用户再A库上更新了一条记录，在更新完这一刻，A与B、C库的数据是不一致的。这种情况在分布式数据库上是必然存在的。这就是Partition tolerance（分区容错性） 当数据不一致的时候，必定是满足分区容错性，如果不满足，那么这个就不是一个可靠的分布式系统。 然而在数据不一致的情况下，系统要么选择优先保持数据一致性，这样的话。系统首先要做的是数据的同步操作，此时需要暂停系统的响应。这就是满足CP。 若系统优先选择可用性，那么在数据不一致的情况下，会在第一时间放弃一致性，让整体系统依然能运转工作。这就是AP。 所以，分布式系统在通常情况下，要不就满足CP，要不就满足AP。 那么有没有满足CA的呢？有，当分布式节点为1的时候，不存在P，自然就会满足CA了。 例子上面说到，分区容错性是分布式系统中必定要满足的，需要权衡的是系统的一致性与可用性。那么常见的分布式系统是基于怎样的权衡设计的。 Zookeeper保证CP。当主节点故障的时候，Zookeeper会重新选主。此时Zookeeper是不可用的，需要等待选主结束才能重新提供注册服务。显然，Zookeeper在节点故障的时候，并没有满足可用性的特性。在网络情况复杂的生产环境下，这样的的情况出现的概率也是有的。一旦出现，如果依赖Zookeeper的部分会卡顿，在大型系统上，很容易引起系统的雪崩。这也是大型项目不选Zookeeper当注册中心的原因。 Eureka保证AP。在Eureka中，各个节点是平等的，它们相互注册。挂掉几个节点依然可以提供注册服务的（可以配置成挂掉的比例），如果连接的Eureka发现不可用，会自动切换到其他可用的几点上。另外，当一个服务尝试连接Eureka发现不可用的时候，切换到另外一个Eureka服务上，有可能由于故障节点未来得及同步最新配置，所以这个服务读取的数据可能不是最新的。所以当不要求强一致性的情况下，Eureka作为注册中心更为可靠。 Git其实Git也是也是分布式数据库。它保证的是CP。很容易猜想到，云端的Git仓库于本地仓库必定是要保证数据的一致性的，如果不一致会先让数据一致再工作。当你修改完本地代码，想push代码到Git仓库上时，假如云端的HEAD与本地的HEAD不一致的时候，会先同步云端的HEAD到本地HEAD，再把本地的HEAD同步到云端。最终保证数据的一致性。 更多技术文章、精彩干货，请关注个人博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>计算机理论</category>
      </categories>
      <tags>
        <tag>CAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java异步编程——深入源码分析FutureTask]]></title>
    <url>%2Fjava-thread-future%2F</url>
    <content type="text"><![CDATA[Java的异步编程是一项非常常用的多线程技术。 之前通过源码详细分析了ThreadPoolExecutor《你真的懂ThreadPoolExecutor线程池技术吗？看了源码你会有全新的认识》。通过创建一个ThreadPoolExecutor，往里面丢任务就可以实现多线程异步执行了。 但之前的任务主要倾向于线程池，并没有讲到异步编程方面的内容。本文将通过介绍Executor+Future框架（FutureTask是实现的核心），来深入了解下Java的异步编程。 万事从示例开始，我们先通过示例Demo有一个直观的印象，再深入去了解概念与原理。 使用示例Demo：使用上比较简单， 运行结果：12345678910111213任务1异步执行：0任务2异步执行：0任务2异步执行：1...任务2异步执行：45同步代码任务2异步执行：24...任务1异步执行：199任务1:执行完成...任务2异步执行：199任务2：执行完成 假若你多次执行这个程序，会发现结果大大的不一样，因为两个任务和同步代码是异步由多条线程执行的，打印的结果当然是随机的。 回顾这个Demo做了什么， 构建了一个线程池 往线程池里面丢两个需要执行的任务 最后获取这两个任务的结果 其中第二点是异步执行两个任务，这两个任务和主线程分别是用了三个线程并发执行的，第三点是在主线程中同步等待两个任务的结果。 很容易看出来，异步编程的好处就在于可以让不相干的任务异步执行，不阻塞主线程。若是主线程需要异步执行的结果，此时再去等待结果会更加高效，提高程序的执行效率。 下面来看看整个流程的实现原理。 源码分析一般在实际项目中，都会有配置有自己的线程池，建议大家在用异步编程时，配置一个专用的线程池，做好线程隔离，避免异步线程影响到其他模块的工作。Demo中为了方便，直接调用Exectors的方法生成一个临时的线程池，日常不建议使用。 我们从这个ExecutorService.submit()方法入手，看看整体实现。 ExecutorService.submit()定义一个接口。这个接口接收一个Callable参数（执行的任务），返回一个Future（计算结果）。 Callable，相当于一个需要执行的任务。它不接收任何参数，可以返回结果，可以抛出异常。相类似的还有Runnable，它也是不接收，不同点在于它不返回结果，也不抛异常，异常需要在任务内部处理。总结来说Callable更像一个方法的调用，Runnable则是一个不需要理会结果的调用。在JDK 8以后，它们都可以通过Lamda表达式写法去替代内部类的写法（详见Demo）。 Future，一个异步计算的结果。调用get()方法可以得到对应的计算结果，如果调用时没有异步计算完，会阻塞等待计算的结果。同时它还提供方法可以尝试取消任务的执行。 看回ExecutorService.submit()的实现，代码在实现类AbstractExecutorService中。除了它接口的实现，还提供了两种变形。原来接口只接收Callable参数，实现类中还新增了接收Runnable参数的。 如果看过之前写的《你真的懂ThreadPoolExecutor线程池技术吗？看了源码你会有全新的认识》，应该了解ThreadPoolExecutor执行任务是可以调用execute()方法的。而这里面submit()方法则是为Callable/Runnable加多一层FutureTask，从而使执行结果有一个存放的地方，同时也添加一个可以取消的功能。原本的execute()只能执行任务，不会返回结果的，具体实现原理可以看看之前的文章分析。 FutureTask是RunnableFuture的实现。而RunnableFuture是继承Future和Runnable接口的，定义run()接口。因为FutureTask有run()接口，所以可以直接用一个Callable/Runnable创建一个FutureTask单独执行。但这样并没有异步的效果，因为没有启用新的线程去跑，而是在原来的线程阻塞执行的。 到这里我们清楚知道了，submit()方法重点是利用Callable/Runnable创建一个FutureTask，然后多线程执行run()方法，达到异步处理并且得到结果的效果。而FutureTask的重点则是run()方法如何持有保存计算的结果。 FutureTask.run()首先判断futureTask对象的state状态，如果不是NEW的话，证明已经开始运行过了，则退出执行。同时futureTask对象通过CAS，把当前线程赋值给变量runner（是Thread类型，说明对象使用哪个线程执行的），如果CAS失败则退出。 外层try{}代码块中，对callable判空和state状态必须是NEW。内层try{}代码真正调用callable，开始执行任务。若执行成功，则把ran变量设为true，保存结果在result变量中，证明已跑成功过了；若抛异常了，则设为false，result为空，并且调用setException()保存异常。最后如果ran为true的话，则调用set()保存result结果。 看下setException()和set()的实现。两者的基本流程一样，CAS置换状态，保存结果在outcome变量道中，但setException()保存的结果类型固定是Throwable。另外一个不同在于最终state状态，一个是EXCEPTION，一个是NORMAL。 这两个方法最后都调用了finishCompletion()。这个方法主要是配合线程池唤醒下一个任务。 FutureTask.get()从上面run()方法得知，最后执行的结果放在了outcome变量中。那最终怎么从其中取出结果来，我们来看看get()方法。从源码可知，get()方法分两步。第一步，先判断状态，如果计算为完成，则需要阻塞地等待完成。第二步，如果完成了，则调用report()方法获取结果并返回。 先看看awaitDone()阻塞等待完成。该方法可以选用超时功能。在自旋的for()循环中， 先判断是否线程被中断，中断的话抛异常退出。 然后开始判断运行的state值，如果state大于COMPLETING，证明计算已经是终态了，此时返回终态变量。 若state等于COMPLETING，证明已经开始计算，并且还在计算中。此时为了避免过多的CPU时间放在这个for循环的自旋上，程序执行Thread.yield()，把线程从运行态降为就绪态，让出CPU时间。 若以上状态都不是，则证明state为NEW，还没开始执行。那么程序在当前循环现在会新增一个WaitNode，在下一个循环里面调用LockSupport.park()把当前线程阻塞。当run()方法结束的时候，会再次唤醒此线程，避免自旋消耗CPU时间。 如果选用了超时功能，在阻塞和自旋过程中超时了，则会返回当前超时的状态。 第二步的report()方法比较简单。 如果状态是NORMAL，正常结束的话，则把outcome变量返回； 如果是取消或者中断状态的，则抛出取消异常； 如果是EXCEPTION，则把outcome当作异常抛出（之前setException()保存的类型就是Throwable）。从而整个get()会有一个异常抛出。 总结至此我们已经比较完整地了解Executor+Future的框架原理了，而FutureTask则是该框架的主要实现。下面总结下要点 Executor.sumbit()方法异步执行一个任务，并且返回一个Future结果。 submit()的原理是利用Callable创建一个FutureTask对象，然后执行对象的run()方法，把结果保存在outcome中。 调用get()获取outcome时，如果任务未完成，会阻塞线程，等待执行完毕。 异常和正常结果都放在outcome中，调用get()获取结果或抛出异常。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Java多线程</category>
      </categories>
      <tags>
        <tag>源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一文了解JVM全部垃圾回收器，从Serial到ZGC]]></title>
    <url>%2Fjvm-gc-collector%2F</url>
    <content type="text"><![CDATA[《对象搜索算法与回收算法》介绍了垃圾回收的基础算法，相当于垃圾回收的方法论。接下来就详细看看垃圾回收的具体实现。 上文提到过现代的商用虚拟机的都是采用分代收集的，不同的区域用不同的收集器。常用的7种收集器，其适用的范围如图所示 Serial、ParNew、Parallel Scavenge用于新生代；CMS、Serial Old、Paralled Old用于老年代。并且他们相互之间以相对固定的组合使用（具体组合关系如上图）。G1是一个独立的收集器不依赖其他6种收集器。ZGC是目前JDK 11的实验收集器。 下面来看看各个收集器的特性 Serial收集器Serial，是单线程执行垃圾回收的。当需要执行垃圾回收时，程序会暂停一切手上的工作，然后单线程执行垃圾回收。 因为新生代的特点是对象存活率低，所以收集算法用的是复制算法，把新生代存活对象复制到老年代，复制的内容不多，性能较好。单线程地好处就是减少上下文切换，减少系统资源的开销。但这种方式的缺点也很明显，在GC的过程中，会暂停程序的执行。若GC不是频繁发生，这或许是一个不错的选择，否则将会影响程序的执行性能。对于新生代来说，区域比较小，停顿时间短，所以比较使用。 ParNew收集器ParNew同样用于新生代，是Serial的多线程版本，并且在参数、算法（同样是复制算法）上也完全和Serial相同。 Par是Parallel的缩写，但它的并行仅仅指的是收集多线程并行，并不是收集和原程序可以并行进行。ParNew也是需要暂停程序一切的工作，然后多线程执行垃圾回收。因为是多线程执行，所以在多CPU下，ParNew效果通常会比Serial好。但如果是单CPU则会因为线程的切换，性能反而更差。 Parallel Scavenge收集器新生代的收集器，同样用的是复制算法，也是并行多线程收集。与ParNew最大的不同，它关注的是垃圾回收的吞吐量。 这里的吞吐量指的是 总时间与垃圾回收时间的比例。这个比例越高，证明垃圾回收占整个程序运行的比例越小。 Parallel Scavenge收集器提供两个参数控制垃圾回收的执行： -XX:MaxGCPauseMillis，最大垃圾回收停顿时间。这个参数的原理是空间换时间，收集器会控制新生代的区域大小，从而尽可能保证回收少于这个最大停顿时间。简单的说就是回收的区域越小，那么耗费的时间也越小。所以这个参数并不是设置得越小越好。设太小的话，新生代空间会太小，从而更频繁的触发GC。 -XX:GCTimeRatio，垃圾回收时间与总时间占比。这个是吞吐量的倒数，原理和MaxGCPauseMillis相同。 因为Parallel Scavenge收集器关注的是吞吐量，所以当设置好以上参数的时候，同时不想设置各个区域大小（新生代，老年代等）。可以开启-XX:UseAdaptiveSizePolicy参数，让JVM监控收集的性能，动态调整这些区域大小参数。 Serial Old收集器老年代的收集器，与Serial一样是单线程，不同的是算法用的是标记-整理（Mark-Compact）。因为老年代里面对象的存活率高，如果依旧是用复制算法，需要复制的内容较多，性能较差。并且在极端情况下，当存活为100%时，没有办法用复制算法。所以需要用Mark-Compact，以有效地避免这些问题。 Parallel Old收集器老年代的收集器，是Parallel Scavenge老年代的版本。其中的算法替换成Mark-Compact。 CMS收集器CMS，Concurrent Mark Sweep，同样是老年代的收集器。它关注的是垃圾回收最短的停顿时间（低停顿），在老年代并不频繁GC的场景下，是比较适用的。 命名中用的是concurrent，而不是parallel，说明这个收集器是有与工作执行并发的能力的。MS则说明算法用的是Mark Sweep算法。 来看看具体地工作原理。CMS整个过程比之前的收集器要复杂，整个过程分为四步： 初始标记（initial mark），单线程执行，需要“Stop The World”，但仅仅把GC Roots的直接关联可达的对象给标记一下，由于直接关联对象比较小，所以这里的速度非常快。 并发标记（concurrent mark），对于初始标记过程所标记的初始标记对象，进行并发追踪标记，此时其他线程仍可以继续工作。此处时间较长，但不停顿。 重新标记（remark），在并发标记的过程中，由于可能还会产生新的垃圾，所以此时需要重新标记新产生的垃圾。此处执行并行标记，与用户线程不并发，所以依然是“Stop The World”，时间比初始时间要长一点。 并发清除（concurrent sweep），并发清除之前所标记的垃圾。其他用户线程仍可以工作，不需要停顿。 由于最耗费时间的并发标记与并发清除阶段都不需要暂停工作，所以整体的回收是低停顿的。 由于CMS以上特性，缺点也是比较明显的， Mark Sweep算法会导致内存碎片比较多 CMS的并发能力依赖于CPU资源，所以在CPU数少和CPU资源紧张的情况下，性能较差 并发清除阶段，用户线程依然在运行，所以依然会产生新的垃圾，此阶段的垃圾并不会再本次GC中回收，而放到下次。所以GC不能等待内存耗尽的时候才进行GC，这样的话会导致并发清除的时候，用户线程可以了利用的空间不足。所以这里会浪费一些内存空间给用户线程预留。 有人会觉得既然Mark Sweep会造成内存碎片，那么为什么不把算法换成Mark Compact呢？ 答案其实很简答，因为当并发清除的时候，用Compact整理内存的话，原来的用户线程使用的内存还怎么用呢？要保证用户线程能继续执行，前提的它运行的资源不受影响嘛。Mark Compact更适合“Stop the World”这种场景下使用。 G1收集器G1，Garbage First，在JDK 1.7版本正式启用，是当时最前沿的垃圾收集器。G1可以说是CMS的终极改进版，解决了CMS内存碎片、更多的内存空间登问题。虽然流程与CMS比较相似，但底层的原理已是完全不同。 高效益优先。G1会预测垃圾回收的停顿时间，原理是计算老年代对象的效益率，优先回收最大效益的对象。 堆内存结构的不同。以前的收集器分代是划分新生代、老年代、持久代等。 G1则是把内存分为多个大小相同的区域Region，每个Region拥有各自的分代属性，但这些分代不需要连续。 这样的分区可以有效避免内存碎片化问题。 但是这样同样会引申一个新的问题，就是分代的内存不连续，导致在GC搜索垃圾对象的时候需要全盘扫描找出引用内存所在。 为了解决这个问题，G1对于每个Region都维护一个Remembered Set，用于记录对象引用的情况。当GC发生的时候根据Remembered Set的引用情况去搜索。 两种GC模式： Young GC，关注于所有年轻代的Region，通过控制收集年轻代的Region个数，从而控制GC的回收时间。 Mixed GC，关注于所有年轻代的Region，并且加上通过预测计算最大收益的若干个老年代Region。 整体的执行流程： 初始标记（initial mark），标记了从GC Root开始直接关联可达的对象。STW（Stop the World）执行。 并发标记（concurrent marking），并发标记初始标记的对象，此时用户线程依然可以执行。 最终标记（Remark），STW，标记再并发标记过程中产生的垃圾。 筛选回收（Live Data Counting And Evacuation），评估标记垃圾，根据GC模式回收垃圾。STW执行。 在Region层面上，整体的算法偏向于Mark-Compact。因为是Compact，会影响用户线程执行，所以回收阶段需要STW执行。 令人惊叹的ZGC在JDK 11当中，加入了实验性质的ZGC。它的回收耗时平均不到2毫秒。它是一款低停顿高并发的收集器。 ZGC几乎在所有地方并发执行的，除了初始标记的是STW的。所以停顿时间几乎就耗费在初始标记上，这部分的实际是非常少的。那么其他阶段是怎么做到可以并发执行的呢？ ZGC主要新增了两项技术，一个是着色指针Colored Pointer，另一个是读屏障Load Barrier。 着色指针Colored PointerZGC利用指针的64位中的几位表示Finalizable、Remapped、Marked1、Marked0（ZGC仅支持64位平台），以标记该指向内存的存储状态。相当于在对象的指针上标注了对象的信息。注意，这里的指针相当于Java术语当中的引用。 在这个被指向的内存发生变化的时候（内存在Compact被移动时），颜色就会发生变化。 在G1的时候就说到过，Compact阶段是需要STW，否则会影响用户线程执行。那么怎么解决这个问题呢？ 读屏障Load Barrier由于着色指针的存在，在程序运行时访问对象的时候，可以轻易知道对象在内存的存储状态（通过指针访问对象），若请求读的内存在被着色了。那么则会触发读屏障。读屏障会更新指针再返回结果，此过程有一定的耗费，从而达到与用户线程并发的效果。 把这两项技术联合下理解，引用R大（RednaxelaFX）的话 与标记对象的传统算法相比，ZGC在指针上做标记，在访问指针时加入Load Barrier（读屏障），比如当对象正被GC移动，指针上的颜色就会不对，这个屏障就会先把指针更新为有效地址再返回，也就是，永远只有单个对象读取时有概率被减速，而不存在为了保持应用与GC一致而粗暴整体的Stop The World。 ZGC虽然目前还在JDK 11还在实验阶段，但由于算法与思想是一个非常大的提升，相信在未来不久会成为主流的GC收集器使用。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>JVM深入</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[必知必会JVM垃圾回收——对象搜索算法与回收算法]]></title>
    <url>%2Fjvm-gc-base%2F</url>
    <content type="text"><![CDATA[垃圾回收（GC）是JVM的一大杀器，它使程序员可以更高效地专注于程序的开发设计，而不用过多地考虑对象的创建销毁等操作。但是这并不是说程序员不需要了解GC。GC只是Java编程中一项自动化工具，任何一个工具都有它适用的范围，当超出它的范围的时候，可能它将不是那么自动，而是需要人工去了解与适应地适用。 拥有一定工作年限的程序员，在工作期间肯定会经常碰到像内存溢出、内存泄露、高并发的场景。这时候在应对这些问题或场景时，如果对GC不了解，很可能会成为个人的发展瓶颈。 接下来的两文将详细学习下JVM中垃圾回收（GC）的各个知识要点。本文先从GC的算法开始先了解，铺垫好基础，下一篇再详细讲JVM具体的GC实现。 GC对象搜索算法垃圾回收，第一件事就是要搞清楚哪些东西是垃圾，而后才能对这些垃圾进行回收。 那么有什么办法识别对象是否为无用的垃圾呢？狭义地，怎么判断对象是否没被引用呢？ 通常有以下两种算法去识别判断 引用计数算法这个算法非常简单。给对象一个计数器，每当这个对象被引用了，计数器值加一；引用失效，则减一。但这个对象计数值为0的时候，证明是无用对象，可以被GC程序回收掉。这种算法比较广泛应用在一些脚本语言上，如FLASH、PYTHON等。但是引用计数算法无法解决对象间相互引用的问题。当a对象引用了b对象，b对象也引用了a对象，这样a、b两个对象的计数器值都不会为0，即使这两个对象都被其他对象所引用，最终导致这些对象一直无法被回收。这种情况往往会出现在比较复杂的编程语言中。 可达性分析算法可达性分析算法（GC roots算法），广泛应用于主流的商用语言。设置一个根节点，从图论角度来看，只要从该节点可达一个对象，证明这个对象是存活的（被引用）。 通常地，GC会包含以下区域的对象： 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中JNI（即一般说的Native方法）引用的对象； 垃圾回收算法了解完垃圾是怎么找出来后，接下来看看它们是怎么被清除的。以下介绍几种清除的算法。 标记-清除算法（Mark-Sweep）标记-清除，顾名思义，先标记垃圾，再清除。它是GC最基础的算法，后续很多算法都是基于它上面去改进的。标记的过程在上面搜索GC对象已经介绍过了。被标记的对象，在统一GC的时候会把标记的对象清除掉。这个算法比较简单，不做过多赘述。 这个算法有一个很明显的缺点，就是在垃圾回收后会产生大量不连续的碎片空间，导致程序要申请较大的对象时常无法找到合适的内存空间，迫使再次GC。 复制算法复制算法的存在，正是为了解决内存碎片问题。并且这个算法也是分代算法的基础。 将内存分为大小相等的两块，每次程序只使用其中一块，当GC发生的时候，把存活的对象复制到另外一块内存中，整齐的排列，然后清空原来的那块内存。可以看到，这种算法有点新生代转移到老年代的感觉。 缺点： 把内存可使用的空间减少了一半，造成空间的浪费。 对象存活数量较多的时候，复制性能比较差 这种缺点，在老年代中，对象存活率比较高的场景下是非常场景间。 标记-整理算法（Mark-Compact）针对复制算法的两个缺点，在老年代一般会用这种标记-整理算法。 把存活的对象移到内存的一段，然后把剩余的空间全部清空掉。 分代收集算法分代算法并不是一个特定的算法，也没有什么新的内容。而是把内存分成多个区域，一般为新生代、老年代等。然后根据不同区域不同的特点，用不同回收算法去回收垃圾。 例如新生代，对象存活率低，比较适用复制算法。老年代存活率高，比较适用Mark-Compact算法。 目前几乎所有的商业虚拟机都是采用分代收集的。具体不同的收集器在下一文再详细说明。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>JVM深入</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从源码入手，一文带你读懂Spring AOP面向切面编程]]></title>
    <url>%2Fspring-aop%2F</url>
    <content type="text"><![CDATA[之前《零基础带你看Spring源码——IOC控制反转》详细讲了Spring容器的初始化和加载的原理，后面《你真的完全了解Java动态代理吗？看这篇就够了》介绍了下JDK的动态代理。 基于这两者的实现上，这次来探索下Spring的AOP原理。虽然AOP是基于Spring容器和动态代理，但不了解这两者原理也丝毫不影响理解AOP的原理实现，因为大家起码都会用。 AOP，Aspect Oriented Programming，面向切面编程。在很多时候我们写一些功能的时候，不需要用到继承这么重的方法，例如对每个方法在执行前打log，在没有AOP的情况下，我们只能对每个方法都写一句打log的语句。如果是一个复杂点的功能，那么将会产生许多重复的代码，而且会对模块之间有更多的耦合。然而，在AOP下，我们只需要通过特定的方法，就能直接切入代码，添加自定义的功能（后续再讲AOP里面的概念点）。下面将从一个简单的示例入手，拆解示例的内容，通过源码分析，一步步带大家读懂AOP的原理实现。 使用示例以下代码不以文字形式展示，若需要代码，可以到github查看完整Demo。Demo：https://github.com/Zack-Ku/spring-aop-demo Spring项目依然是用xml最原始的配置方式，为了只是能简单地阅读原理，否则会多很多自动配置的内容在里面。而AOP的配置用的是注解形式，因为毕竟看起来毕竟清晰，容易理解逻辑。 创建一个Gradle项目，添加对应的Spring与AOP的依赖。（Gradle和Maven类似，都是自动化构建的工具。但与Maven相比，Gradle是基于groovy，采用DSL格式，具有更强的灵活性、简洁性、拓展性。现在连Spring的官方源码都是用Gradle的，可以说是一款面向未来的工具，后续也值得我们深入学习。）创建一个Bean，TestBean。创建AOP的Aspect。然后写一个启动类，测试以上配置运行结果： com.zack.demo.TestBean.getStr()开始执行…getStr():Testing!com.zack.demo.TestBean.getStr()方法结束… Demo：https://github.com/Zack-Ku/spring-aop-demo 示例解析与AOP术语概念看到上面的结果，很容易猜想到，LogAspect作用了在TestBean上，使得每次执行TestBean上的方法时，都会执行对应的方法（before/after）。 LogAspect中带注解@Pointcut的allMethod()，是用来扫描程序中的连接点。当执行一个方法时，命中了连接点，则会根据不同的通知，执行对应的织入代码。在上面例子中，执行getStr()前会执行LogAspect中的before()，执行getStr()后会执行LogAspect中的after()。 具体的通知包含 @Before，前置通知，执行方法前执行 @AfterReturn，返回通知，正常返回方法后执行 @After，后置通知，方法最终结束后执行，相当于finaly @Around，环绕通知，围绕整个方法 @AfterThrowing，异常通知，抛出异常后执行 开发者在命中连接点时，可以通过以上不同的通知，执行对应方法。这就是AOP中的Advisor。 以上的内容其实已经把AOP核心的概念都已经点出来了，我们再深入具体的认识下其中的术语， Aspect，切面，一个关注点的模块。 例子中，LogAspect就是切面。 JoinPoint， 连接点，程序执行中的某个点，某个位置。 例子中，testBean.getStr()是连接点。 PointCut，切点，切面匹配连接点的点，一般与切点表达式相关，就是切面如何切点。 例子中，@PointCut注解就是切点表达式，匹配对应的连接点 Advice，通知，指在切面的某个特定的连接点上执行的动作。 例子中，before()与after()方法中的代码。 TargetObject，目标对象，指被切入的对象。 例子中，从ctx中取出的testBean则是目标对象。 Weave，织入，将Advice作用在JoinPoint的过程。 以上概念看起来可以还比较难懂，可以通过以下一图（来源于网络）来理解 请各位读者和各位程序员，在阅读源码的时候，一定要先搞清楚基本概念，和一定一定要知道对应概念的英文，否则在看源码的时候，根本对不上号，使理解难度大大提高。因为源码都是英文写的。 至此AOP的基本使用和概念相信大家都有一定的了解，下面开始从源码入手，去探索整个Spring AOP的实现。 源码分析上面的例子之所以能完成AOP的代理，只因为Spring的xml配置里面加了这一句 &lt; aop : aspectj-autoproxy / > 加上了这一个配置，使得整个Spring项目拥有了AOP的功能。全局搜索下aspectj-autoproxy这个字段，可以发现，是这个类AspectJAutoProxyBeanDefinitionParser解析了这个元素。 其中的parse方法调用的是AopNamespaceUtils类中的registerAspectJAnnotationAutoProxyCreatorIfNecessary。这个方法作用是初始化一个AOP专用的Bean，并且注册到Spring容器中。解析这三个操作， 第一句，注册一个AnnotationAwareAspectJAutoProxyCreator（称它为自动代理器），这个Creator是AOP的操作核心，也是扫描Bean，代理Bean的操作所在。 第二句，解析配置元素，决定代理的模式。其中有JDK动态代理，还有CGLIB代理，这部分后续会再细讲。 第三句，作为系统组件，把Creator这个Bean，放到Spring容器中。让Spring实例化，启动这个Creator。 自动代理器下面我们来细看AnnotationAwareAspectJAutoProxyCreator是怎么对Bean做AOP的。 AnnotationAwareAspectJAutoProxyCreator的父类AbstractAutoProxyCreator，里面实现了BeanPostProceesor接口的postProcessAfterInitialization方法（该方法在一个Bean加载到Spring后会执行）。关联注释描述可知，当一个bean加载完后，执行了该方法，会生成一个新的代理对象，返回context中加载。 下面重点看其中的wrapIfNecessary方法。讲述了整个AOP的核心流程，是Spring AOP最最最核心的代码所在。看到红框的两个核心方法，可以知道，先从刚加载的Bean中扫描出所有的advice和advisor，然后用它来创建一个代理对象。 获取Advisor先看如何扫描出advice和advisor。一步步Debug getAdvicesAndAdvisorsForBean()，找到BeanFactoryAspectJAdvisorsBuilder中的buildAspectJAdvisors方法。该方法就是找出Spring容器中存在的AspectBean，然后返回所有AspectBean中的Advisor。示例中，LogAspect就是AspectBean，然后LogAspect中的before和after方法就是Advisor。所以最终返回了LogAspect中的Advisor（before和after）。 创建代理拿到了所有的Advisor后，就进入了创建代理的流程了createProxy()。这些入参，对比上一篇讲过的动态代理，其实非常相似。 beanClass，加载到Spring，触发AOP的bean类 targetSource，目标对象，示例中则是从ctx中取出的testBean specificInterceptors，指定Advisor，示例中则是before和after的方法。 下面来具体看下代理的过程代码可以概括为，创建一个proxyFactory对象，然后把上面的参数都丢到这个这个工厂里，最后从proxyFactory获取一个代理对象。 来看看ProxyFactory的getProxy方法是怎么生成代理对象的。 Debug该方法，可以在DefaultAopProxyFactory中createAopProxy看到工厂会根据配置与目标对象的类型，选择用JDK动态代理（参考《你真的完全了解Java动态代理吗？看这篇就够了》）还是CGLIB的代理（CGLIB具体在后续讲）。 代理后的对象放回ctx中，然后当程序执行的时候，会直接调用这个代理类。 至此整个AOP的代理流程就结束了。下面来了解下CGLIG代理与JDK代理的不同 CGLIB与JDK代理区别CGLIB（Code Generation Library）是一个强大的，高性能，高质量的Code生成类库。它可以在运行期扩展Java类与实现Java接口。Hibernate支持它来实现PO(Persistent Object 持久化对象)字节码的动态生成。 回顾下JDK代理，JDK代理需要一组需要实现的接口，然后通过这些接口获取构造方法，用这个构造方法和InvocationHandler，实例化一个对象出来。所以JDK的方式是基于接口的。 而CGLIB的代理是基于类的，用目标类生成一个子类，子类重写父类的方法，从而达到动态代理的效果。CGLIB的使用和实现等后面有机会再详细介绍。目前暂时只要理解两者不同的使用场景就足够了。 总结回顾下Spring AOP的流程 Spring加载自动代理器AnnotationAwareAspectJAutoProxyCreator，当作一个系统组件。 当一个bean加载到Spring中时，会触发自动代理器中的bean后置处理 bean后置处理，会先扫描bean中所有的Advisor 然后用这些Adviosr和其他参数构建ProxyFactory ProxyFactory会根据配置和目标对象的类型寻找代理的方式（JDK动态代理或CGLIG代理） 然后代理出来的对象放回context中，完成Spring AOP代理 相信大家通过阅读本文，对Spring的AOP处理有一定的认识。想更深入地了解，探索每一步，每一行代码的实现，可以下载Demo源码，一步步地调试Demo：https://github.com/Zack-Ku/spring-aop-demo 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Spring分析学习</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的完全了解Java动态代理吗？看这篇就够了]]></title>
    <url>%2Fjava-dynamic-proxy%2F</url>
    <content type="text"><![CDATA[之前讲了《零基础带你看Spring源码——IOC控制反转》，本来打算下一篇讲讲Srping的AOP的，但是其中会涉及到Java的动态代理，所以先单独一篇来了解下Java的动态代理到底是什么，Java是怎么实现它的。 动态代理看起来好像是个什么高大上的名词，但其实并没有那么复杂，直接从字面就很容易理解。动态地代理，可以猜测一下它的含义，在运行时动态地对某些东西代理，代理它做了其他事情。先不去搞清楚这个动态代理真正的含义，我们来举个生动的例子来理解下它到底做了什么。 一个例子一个程序员Developer，他会开发code，他调试debug。程序员有很多分类，其中有Java程序员JavaDeveloper，他会开发Java代码，会调试Java代码。但是呢，有个叫Zack的程序员它在开发之前，会祈祷一下，这样他开发的代码就不会有bug。 Zack的这种“特异功能”是后天练出来的，并没有哪种程序员有这种特性。虽然我们也可以定义一个拥有这样特性的程序员，但是拥有各种乱七八糟特性的程序千千万。我们什么时候才能定义完，而能保证不漏呢？ 其实我们没有必要去定义他，因为他是后天养成的，我们应该在这个程序员的成长期去实现这个特性，而不是在他出生之前定义。 我们来看下代码是怎么实现的如果Zack只是一个普通的Java程序员，那么他的开发结果是Zack is coding javaZack is debugging java 但是真正的Zack（代理后）Zack is praying for the code!Zack is coding javaZack’s have no bug！No need to debug! Proxy.newProxyInstance()回看下上面是如何使用动态代理的使用。生成一个实例对象，然后用Proxy的newInstance方法对这个实例对象代理生成一个代理对象。这里有一个非常关键的人，也是比较少人去理解它的。为什么要传zack的类加载和zack的接口呢？有没有留意到zackProxy的类型是Developer接口，而不是一个实现类。因为zack在被代理后生成的对象，并不属于Developer接口的任何一个实现类。但是它是基于Developer接口和zack的类加载代理出来的。 看下newProxyInstance()的接口定义这三个参数具体的含义来看看注解是怎么描述的 loder，选用的类加载器。因为代理的是zack，所以一般都会用加载zack的类加载器。 interfaces，被代理的类所实现的接口，这个接口可以是多个。 h，绑定代理类的一个方法。 loder和interfaces基本就是决定了这个类到底是个怎么样的类。而h是InvocationHandler，决定了这个代理类到底是多了什么功能。所以动态代理的内容重点就是这个InvocationHandler。 InvocationHandler根据注解描述可知，InvocationHandler作用就是，当代理对象的原本方法被调用的时候，会绑定执行一个方法，这个方法就是InvocationHandler里面定义的内容，同时会替代原本方法的结果返回。 InvocationHandler接收三个参数 proxy，代理后的实例对象。 method，对象被调用方法。 args，调用时的参数。 在上面的例子里，如果最后的return语句改成1return method.invoke(proxy, agrs); invoke的对象不是zack，而是proxy，根据上面的说明猜猜会发生什么？是的，会不停地循环调用。因为proxy是代理类的对象，当该对象方法被调用的时候，会触发InvocationHandler，而InvocationHandler里面又调用一次proxy里面的对象，所以会不停地循环调用。并且，proxy对应的方法是没有实现的。所以是会循环的不停报错 动态代理的原理通过上面的讲解，相信大家对动态代理的使用理解得比较深刻了。那动态代理到底是怎么实现的呢，我们来看看源码其中关键的地方。在newProxyInstance()发放中有这样几段。其实大概就是把接口复制出来，通过这些接口和类加载器，拿到这个代理类cl。然后通过反射的技术复制拿到代理类的构造函数（这部分代码在Class类中的getConstructor0方法），最后通过这个构造函数new个一对象出来，同时用InvocationHandler绑定这个对象。 动态代理的使用场景动态代理的好处我们从例子就能看出来，它比较灵活，可以在运行的时候才切入改变类的方法，而不需要预先定义它。 动态代理一般我们比较少去手写，但我们用得其实非常多。在Spring项目中用的注解，例如依赖注入的@Bean、@Autowired，事务注解@Transactional等都有用到，换言之就是Srping的AOP（切面编程）。 这种场景的使用是动态代理最佳的落地点，可以非常灵活地在某个类，某个方法，某个代码点上切入我们想要的内容，就是动态代理其中的内容。所以下一篇我们来细致了解下Spring的AOP到底是怎么使用动态代理的。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Java基础</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>动态代理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零基础带你看Spring源码——IOC控制反转]]></title>
    <url>%2Fspring-ioc%2F</url>
    <content type="text"><![CDATA[本章开始来学习下Spring的源码，看看Spring框架最核心、最常用的功能是怎么实现的。网上介绍Spring，说源码的文章，大多数都是生搬硬推，都是直接看来的观点换个描述就放出来。这并不能说有问题，但没有从一个很好的、容易切入的角度去了解学习。博主来尝试抛弃一些所知，从使用上入手，步步回溯源码去了解学习。 很多人会混乱IOC和DI的两个概念，其实这两者是层面的不同。具体的区别的区别：IOC是DI的原理。依赖注入是向某个类或方法注入一个值，其中所用到的原理就是控制反转。所以说到操作层面的时候用DI，原理层的是说IOC，下文亦同。 对于DI最新使用方法，现在都是建议用Java注解去标识。但是相信笔者，不要用这种方式去看源码。笔者本来是想从Java注解入手去一步步看源码，debug看看发生什么了。但发现更多时间是在调SpringBoot和AOP的源码。在看了一天后，还是换一种思路吧，因为AOP是打算在下一章再讲的。 所以我用XML的方式，搭了一个最简单的Spring项目来学习其中IOC的源码。建议大家把代码拉下来，跟着笔者思路来一起看。源码在此：https://github.com/Zack-Ku/spring-ioc-demo 搭建内容maven的依赖,只添加了spring-context模板，用的是4.3.11版本（部分代码）1234567 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;4.3.11.RELEASE&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 作为Bean的Service（部分代码）12345public class TestBeanServiceImpl implements TestBeanService &#123; public String getBean() &#123; return &quot;a test bean&quot;; &#125;&#125; 配置XML（部分代码）1&lt;bean id=&quot;testBeanService&quot; class=&quot;com.zack.demo.TestBeanServiceImpl&quot;/&gt; 启动类。只是加载了下spring的xml配置，然后从context中拿出Bean，这就是完整IOC的过程了。（部分代码）123456789101112public class Application &#123; public static void main(String[] args) &#123; // 加载xml配置 ApplicationContext context = new ClassPathXmlApplicationContext(&quot;classpath:application.xml&quot;); // IOC获取Bean TestBeanService testBeanService = context.getBean(TestBeanService.class); System.out.println(testBeanService.getBean()); &#125;&#125; 最后启动就能获取这个bean，看到getMessage()打印的内容了。 这样就是一个比较纯粹的Spring-IOC的项目了。我们直接从启动类开始看起 Bean的含义前置先解释下这个Bean的含义，因为会贯穿整个流程。通俗地讲，Bean就是IOC的容器。如上面的例子，将TestBeanService注册到Spring里，那么TestBeanService就是Spring的里面的一个Bean。Demo里面context.getBean()就是从Spring中取出这个Bean，完成控制反转的。 所以我们的重点就是要看看Spring到底是怎么生成管理这些Bean的。 ClassPathXmlApplicationContext启动类中，加载配置的ClassPathXmlApplicationContext肯定就是完成IOC的核心。不知道它到底是怎么做的，怎么入手呢？先来看看它的类图先分析下这个类图， ClassPathXmlApplicationContext类是AbstractApplicationContext抽象类的子类 AbstractApplicationContext类是ApplicaionContext接口的实现。 ApplicaionContext接口集合了非常多的内容，其中和IOC比较相关的就是ListableBeanFactory接口和HierarchicalBeanFactory接口 ListableBeanFactory接口和HierarchicalBeanFactory接口是继承BeanFactory 从此分析可以看出，ClassPathXmlApplicationContext是什么，了解下ApplicaionContext；它怎么和IOC有关，要了解BeanFactory。所以后面我们先来看看ApplicaionContext与BeanFactory。 ApplicationContext从该接口的注解描述可知，ApplicationContext是整个项目的配置，Spring项目在启动或运行的时候都需要依赖到它。 其中Bean管理相关的则是ListableBeanFactory和HierarchicalBeanFactory。 BeanFactoryListableBeanFactory和HierarchicalBeanFactory都是继承BeanFactory的。先看看BeanFactory的文件注解从上图可知，BeanFactory就是获取Bean容器的地方。而且他可以提供单例的对象或者是独立的对象 从这段可以得知，HierarchicalBeanFactory是一个分层的Bean，如果实现了这个接口，所有方法都会经过父类的工厂。所以这个是个拓展的类，暂时先不看它。 接下来看看ListableBeanFactory注解说明这个接口是要实现预先加载Bean的配置，生成好实例，直接管理Bean的实例，而不是来一个请求，生成一个。 好了，以上就是基本的概念和认知，现在带着这些概念，我们回头看看ClassPathXmlApplicationContext的执行流程，看看它到底怎么的生成管理Bean的。 初始化IOC容器从ClassPathXmlApplicationContext的构造函数看，最核心的就是refresh()函数，其他只是设一些值。而这个refresh()是调用父类AbstractApplicationContext中的refresh()。根据它的注解可知它是加载刷新了整个context，并且加载所有Bean定义和创建对应的单例。 看下这个方法做了什么里面有许多步骤，重点看下obtainFreshBeanFactory()（重新获取一个BeanFactory）。它里面有个核心的方法refreshBeanFactory()如果已有BeanFactory，先删除所有Bean，然后关闭BeanFactory。然后创建一个新的ListableBeanFactory，上面说到这个工厂里会预先加载所有的Bean。最后核心的就是loadBeanDefinitions(beanFactory)，它是加载Bean的定义。实现交给了子类。用的是XmlBeanDefinitionReader直接读配置文件加载Bean Definition(Bean定义)到BeanFactory。它里面一步步把xml的配置文件拆解读取，把一个个Bean Definition加载到BeanFactory里。至此，已经有用一个加载好Bean Definition的BeanFactory了。 其他方法也是围绕BeanFactory后置处理和Context的配置准备。内容太多，想更深入了解的话建议顺着以上思路，找到对应代码阅读以下。 依赖注入回到启动类中，看看怎么从context中获取bean的。1context.getBean(TestBeanService.class) 是根据类去拿bean的，当然也可以根据id。其对应的源码实现，在DefaultListableBeanFactory中，上文有说到对应的BeanFactory选型。NamedBeanHolder是里面包含一个实例化的对象，和bean的名字。resolveNamedBean()是怎么拿出Bean的关键。 一步步Debug，可以看到，它是遍历BeanFactory里面维护的beanDefinitionNames和manualSingletonNames成员变量，找出命中的beanName返回。然后拿着这个beanName去找具体的bean实例。这里的代码比较长，在AbstractBeanFactory里面的doGetBean()中实现。大意是先尝试去找手动添加bean的单例工厂里找有没有对应的实例，没有的话就往父类beanFactory里面找，最后没有的话就生成一个。 spring中一个bean是如何加载和如何注入大致如此，更细节的内容，可以自己debug看看源码。 控制反转的优点最后来以我个人观点谈谈控制反转的优点吧。举个例子，我要装修房子，需要门、浴具、厨具、油漆、玻璃等材料。1decorateHouse(Door,BathThing,CookThing,....) 但是我作为一个装修工人，我需要去制造门、制造浴具，合成玻璃油漆吗？不需要，也不关心其建造的过程，对应的会有人去做这些东西。12door = buildDoor();glass = buildGlass(); 所有材料放到建材商城里面，装修工人需要什么材料就去建材商城里面取。 对应Spring的IOC，门、玻璃等材料就是Bean，建材商城就是IOC容器，把材料放到建材商城就是Bean加载，去商城拿材料就是依赖注入的过程。 程序开发发展至今，一个简答的项目或许也要分几个模板，几个人去开发。划分好职责，设计好接口，面向接口编程。每个人只需要完成好自己那部分的工作，依赖调用就可以了。这样做同时有助于降低项目的耦合度，让项目有更好的延伸性。由此Spring的IOC就是基于以上的需求所诞生的。 总结回顾下全文的内容 ApplicationContext是Spring项目的核心配置，项目运行依赖于它，其中包含许多方面的内容。 BeanFactory是Context包含的内容之一，它负责管理Bean的加载，生成，注入等内容。 Spring控制反转为了降低项目耦合，提高延伸性。 本文讲Spring IOC还比较浅显，仅仅讲了如何加载的重点和注入的重点，关于生命周期，BeanFactory的处理由于篇幅问题并没有细讲。有兴趣的读者可以用Demo跑起来，一步步Debug看看。因为Demo基本是最小化的Spring IOC了，所以这个Debug不会太难，很容易就能看清楚整个流程做了什么。 Demo：https://github.com/Zack-Ku/spring-ioc-demo 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Spring分析学习</category>
      </categories>
      <tags>
        <tag>Spring</tag>
        <tag>ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你真的懂ThreadPoolExecutor线程池技术吗？看了源码你会有全新的认识]]></title>
    <url>%2Fjava-thread-threadpoolexecutor%2F</url>
    <content type="text"><![CDATA[Java是一门多线程的语言，基本上生产环境的Java项目都离不开多线程。而线程则是其中最重要的系统资源之一，如果这个资源利用得不好，很容易导致程序低效率，甚至是出问题。 有以下场景，有个电话拨打系统，有一堆需要拨打的任务要执行，首先肯定是考虑多线程异步去执行。假如我每执行一个拨打任务都new一个Thread去执行，当同时有1万个任务需要执行的时候，那么就会新建1万个线程，加上线程各种初始销毁等操作，这个消耗是巨大的。而其实往往实现这些功能的时候，并不是完全需要实时马上完成，只是希望在可控范围内尽量提高执行的并发性能。 因此线程池技术应用而生，Java中最常用的线程池技术就是ThreadPoolExecutor。接下来就整体看看ThreadPoolExecutor的实现。这个类的注解非常多，很多也是重点，所以就不从注解开始看起。先从使用说起，有个概念先。 基本使用12345678910111213141516171819// 核心线程int corePoolSize = 5;// 最大线程int maximumPoolSize = 10;// 线程空闲回收时间int keepAliveTime = 30;// 线程空闲回调时间单位TimeUnit unit = TimeUnit.SECONDS;// 队列大小int queueSize = 20;// 队列BlockingQueue workQueue = new ArrayBlockingQueue&lt;Runnable&gt;(queueSize);ThreadPoolExecutor executor = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue);executor.execute(() -&gt; &#123; // do something 1&#125;);executor.execute(() -&gt; &#123; // do something 2&#125;); 定义好一些必要的参数，构建一个ThreadPoolExecutor对象。然后调用对象的execute()方法即可。参数说明： corePoolSize，线程池保留的最小线程数。如果线程池中的线程少于此数目，则在执行execut()时创建。 maximumPoolSize，线程池中允许拥有的最大线程数。 keepAliveTime、unit，当线程闲置时，保持线程存活的时间。 workQueue，工作队列，存放提交的等待任务，其中有队列大小的限制。 线程管理机制非常多人误解了corePoolSize、maximumPoolSize、workQueue的相互关系。不少人认为无论队列选择什么，corePoolSize和maximumPoolSize一定是有用，定义一定是生效的，其实并不然啊!看下线程基本规则注解说明 默认情况下，线程池在初始的时候，线程数为0。当接收到一个任务时，如果线程池中存活的线程数小于corePoolSize核心线程，则新建一个线程。 如果所有运行的核心线程都都在忙，超出核心线程处理的任务，执行器更多地选择把任务放进队列，而不是新建一个线程。 如果一个任务提交不了到队列，在不超出最大线程数量情况下，会新建线程。超出了就会报错。 另外，如果想在线程初始化时候就有核心线程，可以调用prestartCoreThread()或prestartAllCoreThread()，前者是初始一个，后者是初始全部。 再看看排队策略 直接提交，用SynchronousQueue。特点是不保存，直接提交给线程，如果没没线程，则新建一个。 无限提交，用类似LinkedBlockingQueue无界队列。特点是保存所以核心线程处理不了的任务，队列无上限，最大线程也没用。 有限提交，用类似ArrayBlockingQueue有界队列。特点是可以保存超过核心线程的任务，并且队列也是有上限的。超过上限，新建线程（满了抛错）。更好地保护资源，防止崩溃，也是最常用的排队策略。 从以上规则可以看出来，核心线程数和最大线程数，还有队列结构是相互影响的，如何排队，队列多大，最大线程是多少都是不一定的。 再看看保持存活机制当超过核心线程数的线程，线程池会让该线程保持存活keepAliveTime时间，超过该时间则会销毁该线程。另外默认对非核心线程有效，若想核心线程也适用于这个机制，可以调用allowCoreThreadTimeOut()方法。这样的话就没有核心线程这一说了。 综合以上，线程池在多次执行任务后，会一直维持部分线程存活，即使它是闲置的。这样的目的是为了减少线程销毁创建的开销，下次有个任务需要执行，直接从池子里拿线程就能用了。但核心线程不能维护太多，因为也需要一定开销。最大的线程数保护了整个系统的稳定性，避免并发量大的时候，把线程挤满。工作队列则是保证了任务顺序和暂存，系统的可靠性。线程存活规则的目的和维护核心线程的目的类似，但降低了它的存活的时间。 另外还有拒绝机制，它提供了一些异常情况下的解决方案。 ctl线程状态控制这个ctl变量是整个线程池的核心控制状态。这个ctl代表了两个变量 workerCount，生效的线程数。基本可以理解为存活的线程，但某个时候有暂时性的差异。 runState，线程池的运行状态。其中，ctl（int32位）的低29位代表workerCount，所以最大线程数为(2^29)-1。另外3位表示runState。 runState有以下几种状态： RUNNING：接收新任务，处理队列任务。 SHUTDOWN：不接收新任务，但处理队列任务。 STOP：不接收新任务，也不处理队列任务，并且中断所有处理中的任务。 TIDYING：所有任务都被终结，有效线程为0。会触发terminated()方法。 TERMINATED：当terminated()方法执行结束。 当调用了shutdown()，状态会从RUNNING变成SHUTDOWN，不再接收新任务，此时会处理完队列里面的任务。如果调用的是shutdownNow()，状态会直接变成STOP。当线程或者队列都是空的时候，状态就会变成TIDYING。当terminated()执行完的时候，就会变成TERMINATED。 execute()带着对上面的规则与机制的认识，现在从就这这个入口开始看看源码，到底整个流程是怎么实现的。 如果少于核心线程在跑，用这个任务尝试创建一个新线程。 如果一个任务成功入队，再次检查下线程池状态看是否需要入队，因为可能在入队过程中，状态发送了变化。如果确认入队且没有存活线程，则新建一个空线程。 如果进不了队，则尝试新建一个线程，如果都失败了。拒绝这个task对于第二点最后为什么新建一个线程？很容易猜想到，会有一个轮询的机制让下个task出队，直接利用这个空闲线程。 注释基本解释了所有代码，代码也没什么特别的。其中最主要的还是addWoker()这个方法，下面来看看。 addWoker()先了解下这个方法的整体思路从描述可知，addwoker失败，会在线程池状态不对、线程满了或者线程工厂创建线程池失败时候发生。这个方法比较长，分两段看。先看第一段。retry:这种写法，如果比较少看源码的，应该是前所未见的了。这是个循环的位置标记，是java的语法之一。看回代码，这里面for循环还嵌套里一个for循环，而retry:是标记第一个for循环的，后面break和continue语句都指向到了retry。说明break和continue是都是操作外层的for循环。retry可以是任何变量命名合法的字符。 然后看看外出for循环的if语句这个if判断想要执行到return false;，队列为空是一个必要条件。因为addWork()不单只接收新任务会调用到，处理队列中的任务也会调用到。而前面提到SHUTDOWN状态下还会处理队列中的任务的，所以队列不为空是会让它继续执行下去的。 对于内层的for循环会先判断worker的数据是否符合corePoolSize和maximumPoolSize的定义，不满足则返回失败。然后尝试CAS让workerCount自增，如果CAS失败还是继续自旋去自增，直到成功。除非线程池状态发生了变化，发退回到外层for循环重新执行，判断线程池的状态。 第一段的代码，就是让workerCount在符合条件下自增 第二段代码这段比较好理解，先创建一个Worker对象，这个Worker里面包含一个由线程工厂创建的线程，和一个需要执行的任务（可以为空）。如果线程创建成功了，那么就加一个重入锁去把这个新建的Worker对象放到workers成员变量中，在加入之前需要重新判断下线程池的状态和新建线程的状态。如果worker添加到workers成员变量中，就启动这个新建的线程。最后如果添加失败，则执行addWorkFailed(w)。如果失败了，加锁操作回滚下wokers、workerCount，然后判断下状态看看是否需要终结线程池。 addWorker()大概的流程就这样。 总结对于其他方法，没有什么特别的，在此不再过多的叙述，有兴趣的可以翻翻源码阅读下。回顾总结下上面的核心要点 当核心线程满且忙碌时，线程池倾向于把提交的任务放进队列，而不是新建线程。 根据选择队列的不同，maximumPoolSize不一定有用的。具体有三种不同的策略。 ctl是线程池的核心控制状态，包含的runState线程池运行状态和workCount有效线程数。 retry:是一种标记循环的语法，retry可以是任何变量命名合法字符。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Java多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程——ReentrantReadWriteLock源码阅读]]></title>
    <url>%2Fjava-thread-reentranreedwritelock%2F</url>
    <content type="text"><![CDATA[之前讲了《AQS源码阅读》和《ReentrantLock源码阅读》，本次将延续阅读下ReentrantReadWriteLock，建议没看过之前两篇文章的，先大概了解下，有些内容会基于之前的基础上阅读。这个并不是ReentrantLock简单的升级，而是落地场景的优化，我们来详细了解下吧。 背景JUC包里面已经有一个ReentrantLock了，为何还需要一个ReentrantReadWriteLock呢？看看头注解找点线索。它是ReadWriteLock接口的实现。那看看这个接口怎么说在实际场景中，一般来说，读数据远比写数据要多。如果我们还是用独占锁去锁线程避免线程不安全的话，是非常低效的，而且同时也会失去它的并发性。多线程也没有意义了。所以ReadWriteLock就是解决这个问题所存在的。看回ReentrantReadWriteLock的头注解。ReentrantReadWriteLock依然有公平锁/非公平锁的功能，与ReentrantLock不同在于，前者内部维护了读锁和写锁，在公平/非公平模式下，他们会一起去竞争这个锁资源。上图是两条ReentrantReadWriteLock最核心的规则。 申请读锁。当没有其他写锁占有，或者读锁在队列中排队时间最长的，才能成功 申请写锁。当没有其他线程占有读/写锁的情况下，才能成功 又以上两条规则可以推导出， 写锁比读锁要高级 有读锁占用可以继续申请读锁，但其他线程不能申请写锁 有写锁占用其他线程读写都不能申请 所以扣ReadWriteLock接口的说明，可以让读并发，写独占，提高了程序的并发性。 ReentrantReadWriteLock构成看下ReentrantReadWriteLock的file struture之前看过ReentrantLock源码的同学肯定很熟悉这个结构，看起来相同的都是Sync同步器（AQS的子类），以及它的两个公平/非公平子类。不同的是它还多了ReadLock内部类和WriteLock内部类，以及读写对应的成员变量和方法。并且少了lock()、unlock()等方法，而是把加锁解锁的功能下方给这两个子类，符合ReadWriteLock接口的定义。 Sync内部类虽然ReentrantReadWriteLock和ReentrantLock都有Sync，但其实Sync方法已经很大不同了，看下Sync的结构对比之前ReentrantLock的Sync，最大不同在于它多了**shared()方法，用于共享锁的获取与释放。另外tryReadLock()、tryWriteLock()是给WriteLock和ReadLock内部类使用的。 tryAcquire() 独占锁（写锁）申请上文介绍重入锁说到state代表的是重入的次数，在读写锁的语义下，state代表的读/写占有(重入)的次数。c为state，w为独占重入次数。当有线程占用锁时（c!=0），如果没有写锁（w==0）或者独占线程不是当前线程，返回false获取失败。锁的重入总数超过上限会抛出异常。这里很容易看出来，如果有锁占用的时候，如果只是读锁，依然可以申请成功。这就是读锁的锁升级。当没有线程占用的时候，执行writerShouldBlock()判断是否需要阻塞线程（子类实现自己的条件），不需要则CAS state值，返回成功。 tryAquireShared() 共享锁（读锁）申请读锁申请比写锁申请要复杂，有比较多没接触过的成员变量，判断的语句也比较多。先看看成员变量，从他们各自的变量注解可知 firstReader，是第一个获取读锁的线程 firstReaderHoldCount，是firstReader的计数器。 cachedHoldCounter，最近一个成功获取读锁的线程持有数计数器。 readHolds，当前线程重入读锁次数。ThreadLocal，是线程安全的HoldCounter。 先判断是否有写锁占有，如果写锁不是当前线程，获取读锁失败，退出方法。注意如果写锁是当前线程是可以获取读锁的，因为写锁是独占的，这种情况下是不会有数据与其他线程共享的问题。满足子类条件，也不超过总数，CAS也成功的情况下，如果没有读锁，则设firstReader为当前线程，firstReaderHoldCount为1；如果有读锁，并且也是当前线程申请获取，firstReaderHoldCount自增1；如果有读锁，不是当前线程申请，取上一个成功的缓存计数器，如果这个计数器不是当前线程的，则设为当前的计数器，并且自增，返回成功。（其实就是把缓存计数器置换为当前线程的计数器）最后不满足条件或者CAS失败，执行fullTryAcquireShared(current)返回。至于这些数据算来干嘛，等后面看看release()怎么用。其实这个方法就是用for循环轮询解决CAS丢失和重入失败的问题，具体代码不细过了，有兴趣可以自己找源码看看。 tryRelease() 独占锁（写锁）释放这里又有Condition的踪迹了，大概可以才行到Condition时控制锁的行为的，取消唤醒等操作。另外锁会同时释放读锁和写锁。这个方法比较好理解的，只要是当前线程操作下，持有重入数减去释放数为0就可以释放了，否则失败。 tryReleaseShared() 共享锁（读锁）释放释放读锁，对正在读的线程不会有什么影响，但可以让等待的写线程去开始获取写锁。剩余的内容就是对tryAquireShared()计算的count数值进行释放（自减），如果最终自减为0则释放读锁成功。 WriteLock、ReadLock内部类前面说到ReentrantReadWriteLock的lock()、unlock()操作是分配到Write/ReadLock里面执行的。他们都是Lock接口的实现，所以其实最像ReentrantLock应该是这个两个内部类。而且大体上也没什么差异，也是用Sync的内部类。WriteLock、ReadLock最大的不同就是WriteLock用的独占模式的方法，ReadLock用的是共享模式的方法。具体的代码实现基本就是上面说明的组成，下面介绍下ReentranReadWriteLock的使用。ReentrantLock的时候比较简单，声明一个变量，调用lock()方法即可。123ReentrantLock rl = new ReentrantLock();rl.lock();rl.unlock(); 但ReentranReadWriteLock并不是Lock接口的实现，所以没有这些方法。有的只是writeLock()、readLock()，要先调用这个方法获取应对的锁对象，再调用lock()。12345ReentrantReadWriteLock rwl = new ReentrantReadWriteLock();rwl.readLock().lock();rwl.readLock().unlock();rwl.writeLock().lock();rwl.writeLock().unlock(); 总结回顾下要点 读写锁ReentrantReadWriteLock，是基于多读少写的实际场景，提高并发性 读写锁的Sync添加了共享模式的方法 读写锁内置了两个对象readLock、writeLock，用于实际的加锁解锁 写锁是独占的，不允许其他锁的申请 读锁可以并发重复申请，当有写锁的时候，会发生锁升级 特别地，在此祝福8月27日生日的她。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Java多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程——ReentrantLock源码阅读]]></title>
    <url>%2Fjava-thread-reentrantlock%2F</url>
    <content type="text"><![CDATA[上一章《AQS源码阅读》讲了AQS框架，这次讲讲它的应用类（注意不是子类实现，待会细讲）。ReentrantLock，顾名思义重入锁，但什么是重入，这个锁到底是怎样的，我们来看看类的注解说明ReentrantLock与隐式锁synchronized功能相同，但ReentrantLock更具有扩展性。《锁优化》里提到Java在1.6对隐式锁synchronized做了锁的优化，使其性能与显式锁性能相差无异。所以在两者的选择上，更多的是考虑用法，以及功能上的扩展。ReentrantLock是线程独占的，不能与其他线程共享。所谓的重入，就是当本线程想再次获得锁，不需要重新申请，它本身就已经锁了，即重入该锁。为什么会允许锁重入呢？因为该线程已经拥有锁了，不会受其他线程干扰，那么里面的共享变量就不会因为多线程执行造成线程不安全。相当于代码已经在串行执行了，没必要再申请多余的锁了，而是重入当前的锁。ReentrantLock会提供一个公平锁的模式，如果选择这个模式，会尽量使得获取锁是公平的，先来先得，但不一定严格按顺序。如果选择了公平锁，性能上会比不使用（默认）低一些。没有一定保证顺序，同时也降性能，所以如果没有特别的要求，尽量使用默认的非公平锁。现在基于以上的认识，来看看ReentrantLock的基本实现吧。 ReentrantLock概览ReentrantLock是实现Lock接口的。所以主要的方法就是Lock接口定义的方法，包括lock()、tryLock()、unlock()、newCondition()等。lock()与tryLock()的区别就是前者会一直等到直到获取锁，后者则是尝试在当时获取锁，不会重复去申请获取。这个newCondition()感觉比较突兀，看起来完全不了解有什么用，和Lock有什么关系，我们后面再详细了解。ReentrantLock里面有一个最核心的成员变量，sync。sync的类型就是内部类Sync。它是AQS的子类，也就是说它就是实现ReentrantLock同步的工具。而FairSync和unFairSync则是Sync的子类，封装了是否公平的功能，用于赋值给sync成员变量。 Sync同步器Sync是继承上文所介绍的AQS，是ReentrantLock里面的NonfairSync和FairSync的父类。看注解可以知道，Sync用了AQS的state（状态原子值）来标识锁被持有的数量。在AQS中，tryRelease()是没有定义的，所以在Sync中重写了。先判断下申请解锁的线程是否独占锁的线程，否则抛出异常退出。然计算新的state值，用当前state减去releases值。对于state值和releases值到底是多少，这里可以先留个悬念，但大家可以思考下上面注解的定义也可以大概猜出来。最后判断新state值是否为0，为0则没有线程占用，所以设当前独占线程为空，并且更新state。这里更新state值并不需要用CAS原子操作，因为只有一个线程会占用这个锁，不是这个线程都异常退出了。AQS中核心的tryAcquire()方法并没有在这里实现，因为子类NonfaiSync和FairSync的实现并不一样。但这里同样需要用到nonfairTryAcquire，所以抽象出来了。但为什么同样需要，暂时不得而知，带着问题后面再看看。先判断当前锁的state是否为0，为0则表示没人获取，然后通过CAS更新为acquires值（依然不知道值是多少），同时更新当前线程为锁的独占线程。如果state不为0，则表示有线程已经占有了。但可能占有的线程是当前线程，那么当前的state会加上acquires值。这里很容易就看出来state就是代表重入的次数！所以上面的谜题就解开了，releases,aquires都是代表每次申请的值，在ReentrantLock肯定都是1，他们的计算总值就是原子值state。如果state不为0，也不是被当前线程占用，那么返回false获取失败。 NonfairSync没啥特别的，直接调用Sync的方法。也没做修改。 FairSync公平锁的同步器。只有当递归调用或者没有其他等待者，再或者他自己本身排第一才能获取锁。这话比较绕口，大概意思应该是不停地轮询申请锁，直到自己排到队列的第一才能获取。乍看一看，这个方法基本和父类Sync的nonfairTryAcquire()一样，唯一不同点就是在没有线程占用的时候（state=0），多了个!hasQueuedPredecessors()前置判断。这个方法用来判断是否队列为空，或者当前线程是否在队列的最前面。所以公平锁模式下，想要能获取锁，除非自己排在队列的最前面。综上看，FairSync根本没有调用到nonfairTryAcquire()，为何说子类都需要用到呢？继续留着悬念，后面解答。 @ReservedStackAccess可以看到上面介绍的tryAcquire()和tryRelease()都有@ReservedStackAccess。这个注解到底有什么用？查找了下资料，这个是JEP 270添加的新注解。它会保护被注解的方法，通过添加一些额外的空间，防止在多线程运行的时候出现栈溢出。具体看下图 lock()、tryLock()成员函数ReentrantLock里面的lock()方法是调用成员变量sync的acquire()。无论是否公平锁都是直接调用AQS的acquire()方法，不过就是各自有tryAcuqire()的重写，即上文所说的内容。参数1，是透传给tryAcquire()的，所以这里代表是入锁一次的意思。而tryLock()调用的是成员变量sync的nonfairTryAcquire()。上文说到Sync内部类抽象了这个方法出来，说到子类都会用到，说的正是tryLock()方法需要用到。所以显而易见的，无论是否公平锁，调用tryLock()都是用的非公平锁的方法。为什么呢？因为tryLock()的try只是尝试，无论是否公平，对于方法来说没有必要，只是尝试申请的时候能否获取锁而已。至于其他成员函数，大都是围绕获取线程和队列的状态，没什么特别的，在这里不再赘述，有兴趣的可以看看源码。 总结回顾下要点 ReentrantLock是一个可重入的锁（被当前占用的线程重入）。 它有两种模式公平与非公平，通过NonfairSync和FairSync赋值sync成员变量实现。 两种模式都是AQS的子类，通过重写tryAcquire()区别不同。公平锁多了是否在队列的头的判断。 tryLock()方法没有区分模式，都是一样的。 上文提到的newCondition()还没有涉及到，等后续再起一章节说下这个Condition。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Java多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程——AQS框架源码阅读]]></title>
    <url>%2Fjava-thread-aqs%2F</url>
    <content type="text"><![CDATA[AQS，全称AbstractQueuedSynchronizer，是Concurrent包锁的核心，没有AQS就没有Java的Concurrent包。它到底是个什么，我们来看看源码的第一段注解是怎么说明看完第一段，总结下 AQS是一个同步的基础框架，基于一个先进先出的队列。 锁机制基于一个状态值，它是原子值。 AQS的子类负责定义与操作这个状态值，但必须通过AQS提供的原子操作 AQS剩余的方法就是围绕队列，与线程阻塞唤醒等功能 基于以上概念，我们看看源码到底是这么实现这些功能的 AQS的成员变量 stateprivate volatile int state;该变量标记为volatile，说明该变量是对所有线程可见的。作用在于每个线程改变该值，都会马上让其他线程可见，在CAS（可见锁概念与锁优化）的时候是必不可少的。在AQS类中，不会直接操作这个值，而是交由它的子类去操作和定义他的作用。 Node、head、tailAQS中有一个静态内部类Node，其实现是一个双向链表。head与tail则是这个链表的头尾指针。作用是存储获取锁失败的阻塞线程。同样的，这个链表是会被多个线程操作的，所以它里面的变量多是被标记为volatile，并且操作也要通过CAS等原子方法去执行。Node还有一个模式的属性：独占模式和共享模式。独占模式下，锁是线程独占的，而共享模式下，锁是可以被多个线程占用的。 VarHandler对于大多数需要操作的原子属性，都对应会有一个大写的值，它的类是VarHandler。例如state、head、tail都有对应的VarHandler，STATE、HEAD、TAIL。VarHandler是1.9的新特性，提供了类似于原子操作以及Unsafe操作的功能，里面的原子操作大多是native方法，比较难查看源码。 ConditionObject条件队列，是AQS中一个非常关键内部类。这个名字起非常奇异，让人搞不懂，看它类注释也看不懂说了什么。看看AQS头部注解这个类是为了让子类支持独占模式的。深入看其中的源码实现，其实就是Node在功能性上的封装，最终让子类实现让当前线程怎么独占一个Object锁。await()、dosign()等方法就是让线程阻塞、加入队列、唤醒线程等。AQS框架下基本各种独占的加锁，解锁等操作到最后都是基于这个类实现的。该类是提供给子类去使用的，具体实现等下次说ReentranLock再深入了解。有人可能觉得为什么实现这个内部类，又不用，而是给子类去用，那为什么不放到子类去呢？其实答案，很简单，抽象加模板模式。p.s. 只有独占锁才能配合该类使用。 AQS的成员函数AQS的公用的方法，主要是加锁与解锁方法。以下方法只提供了模板，部分实现还是在子类当中，直接调用会抛出异常。 acquire()尝试获取锁，失败则进入队列。先执行tryAcquire()（子类实现），成功则直接返回，如果是获取锁失败，则执行addWaiter()，通过CAS在双向链表的尾部添加一个新独占节点。然后把节点丢到acquireQueued()中执行。该方法其实就是自旋尝试获取锁或阻塞线程（子类实现决定）。一开始，获取新节点的前驱节点，如果这个节点是head，则证明只有两个节点，此时再次执行tryAcquire()尝试获取锁，若获取成功，则不需要中断，成功结束。如果还是获取失败，则执行shouldParkAfterFailedAcquire()，根据前驱节点状态（子类设值）判断是否继续自旋（当waitStatus为初始值，重复上一步，直到前面的节点一直在减少到前驱节点为head）或者阻塞线程（当waitStatus标记为SIGNAL）最后如果acquireQueued()返回需要阻塞，则执行selfInterrupt()设置线程为中断可以看回acquire()函数的写法，十分的艺术。利用条件判断的短路规则，实现在if()条件内嵌套判断执行语音。一般人(笔者本人)如果要实现这个功能，会这么写 所以下次遇到类似嵌套if条件判断的语句，可以学习下acquire()的这种短路写法。赞👍 acquireInterruptibly()检查线程是否被中断并尝试获取锁，失败则进入队列。线程中断会退出队列。流程基本和acquire()相同。不同点就是，acquireInterruptibly()在自旋获取过程中如果线程是中断的，那么就会抛出异常退出流程，并且放弃锁。doAcquireInterruptibly()方法与acquireQueued()方法非常相似，不同就是前者在中断状态下，不会再继续获取锁。注意最后有cancelAcquire()方法的执行。 tryAcquireNanos()尝试获取锁，失败则进入队列。当超过指定时间或线程中断会退出队列。在acquireInterruptibly()基础上，增加多一个时间判断，超过指定时间，则退出，放弃获取锁。 release()释放当前锁，并唤醒下一个Node。尝试释放锁若释放成功，且waitStatus不为0（证明是SIGNAL的），就会执行unparkSuccessor()，先取消SIGNAL标志，然后找到最近一个需要SIGNAL的节点，并且唤醒它。 **shared()以上方法皆为独占模式，对应都有共享模式的方法。最大的不同其实就是Node的waitStatus值为PROPAGATE。具体流程与独占大体相同，细节留到ReentrantReadWriteLock再细了解。 总结回顾下要点 AQS是一个同步的基础框架，是ReentranLock、ReentranReadWriteLock的父类 AQS原理是维护一个state原子值，通过一个双向链表的队列实现同步。 对于state、与队列的操作都是原子操作，通过VarHandle实现 主要对外方法是加锁与解锁，区别是否中断、超时、共享或独占模式 以上即使AQS的大致内容，可能有些部分难以理解，其实很正常，因为AQS提供的是流程模板与工具，没有实质落地的场景，是比较难理解的。等后面介绍ReentranLock与ReentrantReadWriteLock的时候，就可以更好更全面的了解整体AQS框架了。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码]]></content>
      <categories>
        <category>Java多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程——锁概念与锁优化]]></title>
    <url>%2Fjava-thread-lock-base%2F</url>
    <content type="text"><![CDATA[为了性能与使用的场景，Java实现锁的方式有非常多。而关于锁主要的实现包含synchronized关键字、AQS框架下的锁，其中的实现都离不开以下的策略。 悲观锁与乐观锁 乐观锁。乐观的想法，认为并发读多写少。每次操作的时候都不上锁，直到更新的时候才通过CAS判断更新。对于AQS框架下的锁，初始就是乐观锁，若CAS失败则转化为悲观锁。 悲观锁。悲观的想法，认为并发写多读少。每次操作数据都上锁，即使别人想读也要先获得锁才能读。对于1.6以前的synchronized关键字，则是悲观锁的实现之一。 CAS无锁算法全称为 Compare and Swap。CAS有三个操作数，内存值V，旧预期值（已获得的旧数据）A，修改新值B。当且仅当V与A的值相同（compare），才能把V替换为B（Swap）。其中Java中内存值可以通过volatile关键字标识获取，该关键词可以使变量对所有线程实时可见。CAS算法在锁的应用非常广泛，java中concurrent包的高性能都是基于这个算法，可以说没有CAS，并发包的高性能也就不存在了。 重量级锁悲观锁的一种。互斥使代码执行可以同步，但这种方式成本比较高，涉及到操作系统的调用阻塞，会造成一些系统资源的浪费。1.6以前，在Java中的即是监视器锁，把.java文件编程成.class文件后能看到synchronized关键字就是通过monitorenter和monitorexit这个两个字节码指令来实现的。 轻量级锁由于在没有很多线程竞争的前提下，重量级锁会导致性能资源的浪费。每次判断是否无锁，无锁则建锁记录，有锁通过CAS去尝试获取锁（对比Mark Word）。该过程失败会让锁膨胀为重量级锁。 偏向锁是轻量级锁的优化，适用于无多线程竞争。虽然轻量级锁在可以在较少线程竞争下，减少操作系统调用，减少互斥变量的产生。但在理想情况下，线程很少发生线程竞争，在轻量级锁中，还是会有比较多的CAS操作。在偏向锁中，有一个锁记录（Mark word）标记为偏向，指向当前线程。若该指向不变，则只需要判断记录是否有被切换。如果被切换了，尝试CAS替换指向，后续一直执行同步代码块。当CAS替换指向失败，则说明存在多线程竞争，此时锁会膨胀为轻量级锁。 自旋锁是锁竞争失败后执行的策略之一，对应的有阻塞的锁。在线程竞争锁失败后，阻塞的锁会把线程阻塞，直到有信号唤起才能继续执行线程，此过程会涉及用户态与系统态的转换，产生性能消耗。而自旋锁在锁竞争失败后，会把线程做自旋，避免线程进入阻塞。在自旋过程中，会不断的尝试去竞争锁。但如果线程一直自旋都获取不到锁，也会产生很多CPU的性能消耗，所以也有一个自适应的自旋锁（控制自旋的时间）解决这个问题。 更多技术文章、精彩干货，请关注博客：zackku.com微信公众号：Zack说码![](http://qiniu.zackku.com/%E6%89%AB%E7%A0%81_%E6%90%9C%E7%B4%A2%E8%81%94%E5%90%88%E4%BC%A0%E6%92%AD%E6%A0%B7%E5%BC%8F-%E5%BE%AE%E4%BF%A1%E6%A0%87%E5%87%86%E7%BB%BF%E7%89%88.png]]></content>
      <categories>
        <category>Java多线程</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>多线程</tag>
        <tag>锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM学习记录——GC对象搜索算法]]></title>
    <url>%2Fjvm-gc-object-search%2F</url>
    <content type="text"><![CDATA[Java与C最大的区别或许就是Java拥有垃圾回收（Garbage Collect）功能，可以让开发人员无需过多关心程序中的对象内存如何管理，更专注于业务的开发。但是如果不好好了解GC，也遇到性能瓶颈时很难容易的解决。 GC回收的对象是无用的对象，狭义是没有被引用的对象。如何在内存中找到这些对象，一般有以下两种 引用计数算法这个算法非常简单。给对象一个计数器，每当这个对象被引用了，计数器值加一；引用失效，则减一。但这个对象计数值为0的时候，证明是无用对象，可以被GC程序回收掉。这种算法比较广泛应用在一些脚本语言上，如FLASH、PYTHON等。但是引用计数算法无法解决对象间相互引用的问题。当a对象引用了b对象，b对象也引用了a对象，这样a、b两个对象的计数器值都不会为0，即使这两个对象都被其他对象所引用，最终导致这些对象一直无法被回收。这种情况往往会出现在比较复杂的编程语言中。 可达性分析算法可达性分析算法（GC roots算法），广泛应用于主流的商用语言。设置一个根节点，从图论角度来看，只要从该节点可达一个对象，证明这个对象是存活的（被引用）。通常地，GC会包含以下区域的对象： 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中JNI（即一般说的Native方法）引用的对象； 参考《深入理解Java虚拟机——JVM高级特性与最佳实践》—— 3.2]]></content>
      <categories>
        <category>JVM深入</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring AOP Transactional注解不能用于内部调用问题]]></title>
    <url>%2Fspring-transaction-in-class-problem%2F</url>
    <content type="text"><![CDATA[Spring框架中的Transactional注解是几个比较常用注解之一，作用就是用于事务的管理。如果系统接的Mybatis的数据库，那么Spring的Transactional就会接替sqlSession的生命周期管理。它的用法非常简单，就是把它标注在需要事务的方法上面即可。例如1234@Transactional(rollbackFor = Exception.class)public void createBidOffer() &#123; insertBidAndOffer();&#125; 但是并不是放到任何方法上，它都会生效的。 类中方法调用一个例子，有如下调用逻辑：controller部分代码12345@RequestMapping(&quot;/transaction&quot;)public Response testTransactionAop() &#123; tradeService.createBidOffer(); return new Response();&#125; TradeService部分代码12345678910111213141516171819@Overridepublic void createBidOffer() &#123; .... insertBidAndOffer(); ....&#125;@Transactional(rollbackFor = Exception.class)public void insertBidAndOffer() &#123; insertBid();&#125;private Bid insertBid() &#123; Bid bid = new Bid(); bid.setTotalAmount(100); bid.setRestAmount(100); bidMapper.insert(bid); return bid;&#125; TradeService的对外接口是createBidOffer()，但不想整个方法做事务，只对于insertBidAndOffer()里面的操作做事务。可惜的是以上这种写法是无效的。虽然执行时候不会有任何报错异常，但一旦insertBidAndOffer()方法执行过程中抛出异常，事务是不会生效的，即使方法是public也没用（Transaction注解要求作用于public的方法上）。 @Transactional 的AOP切点Spring AOP其实是与IOC配合使用的，而Spring AOP是用动态代理的技术。也就是说一个类被IOC所注入生成的对象被Spring动态代理成一个新的代理对象。无效的原因其实就很容易理解了，我们在外部调用这个动态代理对象，会在代理的时候增强对象，但在对象的内部调用的时候，调用的还是原来的对象的方法，该方法明细不会被AOP增强。上面的例子，在Spring框架里，TradeService注入到controller中，生成一个代理的tradeService对象，在controller调用tradeService方法的时候，被代理的方法拦截。该方法中会找出这个AOP连接点的Advice，然后切入执行（也就是执行@Transactional）。若是调用对象内容方法的时候，就不会被代理发放拦截的了。下图即为拦截切入点，框内为切入后执行该切点的Advice，事务就在其中执行P.S. 如果该类实现了接口，Java的Proxy做动态代理；如果没有实现，则是CGLIB做的动态代理（以子类的方式）。 解决方法一个原则：事务注解只用于对外的方法上。如果想实现对象内部调用的逻辑，最简单的方法就是，把逻辑抽出来，放到另外一个类中取调用即可。 源码参考Java web 项目脚手架： https://github.com/Zack-Ku/java-web-scaffold]]></content>
      <categories>
        <category>Spring分析学习</category>
      </categories>
      <tags>
        <tag>bug</tag>
        <tag>Spring</tag>
        <tag>transaction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring中Interface与对应Impl Bean扫描冲突问题]]></title>
    <url>%2Fspring-scan-problem%2F</url>
    <content type="text"><![CDATA[最近在做一个SpringBoot的脚手架，熟悉Spring的各种特性与一些第三方工具的使用。在做到Interface与Impl注入到Controller调用的时候报Bean冲突问题。 问题重现定义一个Interface文件 – GreetingService 1234567package com.zackku.service.hello.service;import ...public interface GreetingService &#123; List&lt;Greeting&gt; findGreats(String content, Integer offset, Integer rows);&#125; 对应的Impl实现 – GreetingServiceImpl123456789101112131415161718192021package com.zackku.service.hello.service.impl;import ...@Servicepublic class GreetingServiceImpl implements GreetingService &#123; private static final String TEMPLATE = &quot;this is %s!&quot;; private GreetingMapper greetingMapper; @Autowired public GreetingServiceImpl(GreetingMapper greetingMapper) &#123; this.greetingMapper = greetingMapper; &#125; @Override public List&lt;Greeting&gt; findGreats(String content, Integer offset, Integer rows) &#123; List&lt;Greeting&gt; greets = greetingMapper.find(content, offset, rows); return greets; &#125;&#125; 注入调用方 – GreetingController123456789101112131415161718192021package com.zackku.api.hello.controller;import ...@RestController@RequestMapping(&quot;/greeting&quot;)public class GreetingController &#123; private GreetingService greetingService; @Autowired public GreetingController(GreetingService greetService) &#123; this.greetingService = greetService; &#125; @RequestMapping(&quot;/findList&quot;) public List&lt;Greeting&gt; find(@RequestParam(value = &quot;content&quot;) String content, @RequestParam(value = &quot;offset&quot;, defaultValue = &quot;0&quot;) Integer offset, @RequestParam(value = &quot;rows&quot;, defaultValue = &quot;2&quot;) Integer rows) &#123; List&lt;Greeting&gt; greets = greetingService.findGreats(content, offset, rows); return greets; &#125;&#125; 以上代码在项目启动时候会报错 Parameter 0 of constructor in com.zackku.api.hello.controller.GreetingController required a single bean, but 2 were found: - greetingServiceImpl: defined in file […/com/zackku/service/hello/service/impl/GreetingServiceImpl.class] - greetingService: defined in file […/com/zackku/service/hello/service/GreetingService.class] 原因分析居然Service的Interface和Impl会冲突？百思不得其解，况且我的Interface也没写注入的注解，即使写了也不应该会冲突。因为Spring的接口注入规则就是只会注入对应接口实现，除非实现有多个。问题找了好久，最后Debug源码发现，在GreetingController 中 greetingService.findGreats的调用居然被MapperProxy代理。这样的话大概猜想到是GreetingService被多次扫描注册了。回看组件扫描的配置1234567891011package com.zackku.api;import ...@ComponentScan(basePackages = &#123;&quot;com.zackku&quot;&#125;)@MapperScan(basePackages = &#123;&quot;com.zackku.service&quot;&#125;)@EnableAutoConfiguration@EnableCachingpublic class ApiApplication extends SpringBootServletInitializer &#123; public static void main(String args[]) &#123; SpringApplication.run(ApiApplication.class, args); &#125;&#125; 再看看MapperScan的扫描原理：MapperScan在指定的basePackages下，扫描所有的interface并注册。 原来MapperScan并没有想象中那么智能去扫指定的Mapper。当ComponentScan与MapperScan扫的路径有交集的时候，里面的interface会被重复的注册使用。最终导致报Bean冲突错误。 解决方法MapperScan注解有一个markerInterface的参数， The scanner will register all interfaces in the base package that also havethe specified interface class as a parent. 也就是说，MapperScan可以只扫描继承该类的接口具体如下： 新增一个MapperInterface接口,空的就行 1public interface MapperInterface &#123;&#125; GreetingMapper继承该接口 1public interface GreetingMapper extends MapperInterface &#123;&#125; 以后新增的Mapper也要继承该接口 修改扫描配置1@MapperScan(basePackages = &#123;&quot;com.zackku.service&quot;&#125;, markerInterface = MapperInterface.class) 这样就完美解决问题了。 源码参考Java web 项目脚手架： https://github.com/Zack-Ku/java-web-scaffold]]></content>
      <categories>
        <category>Spring分析学习</category>
      </categories>
      <tags>
        <tag>bug</tag>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM学习记录——运行时数据区]]></title>
    <url>%2Fjvm-RAM-model%2F</url>
    <content type="text"><![CDATA[JVM规范中规定，运行时数据区（Runtime Data Area）通常包含 程序计数器(Program Counter Register) Java栈(VM Stack) 本地方法栈(Native Method Stack) 方法区(Method Area) 堆(Heap) 从图中可以看出，只有方法区和堆是线程共享的，其他都是线程私有的。通常地，一个Java程序是一个进程，也是Tomcat（或其他服务容器）实例。进程每创建一个线程都有独立的 程序计数器、Java栈、本地方法栈。 堆(Heap)堆是管理内存中最大的一块，负责存储对象实例和数组，在JVM启动的时候创建的，即一个Java进程对应一个堆。里面的对象是所有线程共享的，所以在多线程操作同一个对象时候要注意数据安全性问题。对象会被自动管理，也就是垃圾回收时，回收的就是这块的内存。该区域的大小通过JVM参数 -Xmx、-Xms控制。 虚拟机栈(Virtual Machine Stack)在线程被创建的时候，会同时创建一个该线程私有的栈，即一个线程对应一个栈，生命周期也是相同的。栈用于Java执行时候的内存模型，每执行一个方法都会同时创建栈帧（Stack Frame），每个栈帧里面存储 存储局部变量表、操作数栈、动态链接、方法出口等信息。线程执行，相同于入一个栈帧，操作里面的变量等数据完成这个方法，然后出栈，切换下一个方法。栈大小可通过 -Xss设置。 本地方法栈(Native Method Stack)与虚拟机栈基本相同，只是本地方法栈服务的对象是本地的服务，如一些JVM自身的方法、系统底层的调用等。 方法区(Method Area or Permanent Generation)用于存储一些静态信息。例如类信息，常量，静态变量等，即编译后一些代码数据。方法区的数据也是线程共享的。一些地方也会说到常量池，也是在其中。 程序计数器(Program Counter Register)线程私有，用于保存执行指令的地址，有别于实际的PC寄存器。实际PC寄存器是基于CPU使用的，JVM在执行程序的时候，CPU会变换的执行不同的线程，所以JVM程序在执行不同线程的时候，需要保存各个线程的执行指令的地址，以便恢复执行。]]></content>
      <categories>
        <category>JVM深入</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一个SQL字段定义bug]]></title>
    <url>%2Fdb-defined-bug%2F</url>
    <content type="text"><![CDATA[在今天假期线上反馈一个用户点击app时候出现了bug报错，但这个操作在测试环境已经测试了多遍，代码也是在线上跑了一些日子了，无端端报错也是十分奇怪。查看线上日志发现 1com.mysql.cj.jdbc.exceptions.MysqlDataTruncation: Data truncation: Data too long for column &apos;mobile_belong_city&apos; 手机号码归属地过长无法写入。再查看数据库定义，该字段长度是varchar(10)的。感觉上没啥问题，一个城市名，10个字符应该能写进去，猜想是数据有问题了。然后查看用户的数据，发现这个用户的归属地是1黔西南布依族苗族自治州 共11个字。最后临时先把数据库字段类型改成varchar(20)解决问题。P.S. Mysql在5.0.3后，varchar(10)中的10开始代表字符长度，而不是字节。 总结在定义城市等其他常见字段时候，不要太狭义只定义包含常见的内容。定义范围稍微比常见大一些，或者认真了解下字段包含的内容。例如曼谷的全称长41个字，1天使之城，宏伟之城，永恒的宝石之城，永不可摧的因陀罗之城，世界上赋予九个宝石的宏伟首都，快乐之城，充满著像似统治转世神之天上住所的巍峨皇宫，一座由因陀罗给予、毗湿奴建造的城市 的确很坑爹，但是这个字段的定义不认真对待，最后坑的只有自己。]]></content>
      <categories>
        <category>踩坑记录</category>
      </categories>
      <tags>
        <tag>bug</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器/跳板机登录CMD命令]]></title>
    <url>%2Fjump-servers%2F</url>
    <content type="text"><![CDATA[只要是一家互联网公司，服务器肯定不会少于两台。稍微大一点的，还会有专用的跳板机（堡垒机）作为服务器的跳板。再大一点的，甚至有多台跳板机，几十台服务器。通常的开发拿到这些服务器或跳板机的信息时，都是下载类似SecretCRT的终端软件去登录的，因为这些软件可以保存密码，保存登录所需要的信息。但是作为一名有要求的开发人员，这真的足够方便吗？先要找到这个软件的图标，双击点开，然后找到这个服务器，双击链接上去。好吧，其实也算不上很麻烦，只是不够Geek罢了。一个酷的程序员会怎么做？快捷键Option+Space呼出命令行终端，敲下 1jump-servers a1 一般开发来说，命令行终端是始终打开的，其次使用过得命令是有记录的，所以通常键盘敲两下就可以了。对比点击GUI来说，这个才是程序员的打开方式。下面就来教大家怎么写这个命令（不想从头写可以直接跳到最后一节下载源码修改使用）。 主命令jump-servers用sh命令的case语句控制使用哪个跳板机。语法如下,1234567891011case $变量名 in 模式1） 命令序列1 ;; 模式2） 命令序列2 ;; *） 默认执行的命令序列 ;; esac 很容易的看出来，变量名就是我们输入的命令参数，命令序列就是对应执行的服务器（跳板机登录）命令。完整的文件如下，123456789101112131415161718192021#!/bin/shcase $1 in s1 | server1) echo server1 server1 ;; s2 | server2) echo server2 server2 ;;s3 | server3) echo server3 server3 ;;s4 | server4) echo server4 server4 ;; *) echo not match server! s1/s2/s3/s4esac $1是匹配输入命令时中第一个参数。对应的匹配名，服务器配置文件server1可以自定义。 服务器配置文件servers上面主命令控制具体执行哪里机的命令后，对应执行登录命令。下面的例子是私钥需要对应输入密码的情况（若不需要，可以跳过以下内容）。因为命令需要输入密码进行交互，所以要用到 #!/usr/bin/expect 。完整例子：1234567#!/usr/bin/expectset timeout 3spawn ssh -i ~/.ssh/server1.pem username@127.0.0.1 -p 2222expect &quot;*passphrase*&quot;send &quot;yourpassword\r&quot;interact expect 是监听命令行中出现的字符 send 监听到对应字符后，发送字符 后续假如所有登录方式都是类似可以抽象的，可以把所有服务器信息写到一个配置文件中，然后对应控制匹配，最后传到登录命令中。这样修改添加命令就非常方便了。程序员就是要多动手解决重复的事情，因为这样可以“更懒”。 源码跳板机登录通用命令-Zack-Ku/jump-servers适用对象：win10（powershell），linux，macOS。非以上系统，可以回顾上面教程写出类似的命令。使用方法： 打开server1/2/3/4文件 修改里面的私钥路径 ~/.ssh/server1.pem 修改跳板机登录名 username 修改跳板机ip以及端口 127.0.0.1 ，2222 在send的那名命令中，修改私钥对应的密码 yourpassword 若秘钥对应无密码，删除expect那行开始的剩余代码]]></content>
      <categories>
        <category>工具分享</category>
      </categories>
      <tags>
        <tag>cmd</tag>
        <tag>跳板机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《失控》中的群蜂思维]]></title>
    <url>%2Fbee-out-of-control%2F</url>
    <content type="text"><![CDATA[有一天看到同事桌面上有一本《失控》，估计大家都没看过，讨论到这本神书到底说些什么。恰巧我看过一点，说到里面群蜂思维大概就是就是计算机中的分布式管理，去中心化。有人不相信两者的联系，怎么是去中心化，蜂后不是在管理发送指令吗？其实真相是蜂后只负责繁殖，与物体的接触引路都是工蜂做的（这个可以补下昆虫社会学的简单知识，包括蚁群也是一样的）。这与我们分布式管理非常相似，如果加上近些年非常火的微服务，那更是匹配了。整个集群就看作似群蜂，每个系统有各自的功能，并且相互之间有者各自的交流或联系，最后形成一个对外有机的整体。可以说如果有一个计算机系统的稳定性与效率像群蜂模式一样优秀的模式，那就是分布式管理了。 这本书结合了经济，社会等角度去讨论计算机，可谓是计算机领域的圣经了。但它并不是那么容易读懂，需要慢慢读、细细品味，有空翻翻，工作一段时候后回味下总会有不同的感觉。张小龙也说过，哪位大学生在面试的时候说读懂了《失败》，面试就结束了，可以录用了。可见此书并非等闲之辈。个人建议，微信阅读上看吧，这本书太厚了，携带不方便，偶尔有空看一下，慢慢消化，一直看下去也未必看出门道。]]></content>
      <categories>
        <category>感悟</category>
      </categories>
      <tags>
        <tag>失控</tag>
        <tag>群蜂思维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感悟《硅谷钢铁侠:埃隆·马斯克的冒险人生》]]></title>
    <url>%2Fthink-Elon-Musk%2F</url>
    <content type="text"><![CDATA[最近在读《硅谷钢铁侠:埃隆·马斯克的冒险人生》，惊叹于人与人的差距，真的比人与狗的差距还大。在这分享下书中有趣的内容。 实践的梦想家如果一个人和你说，他想移居火星，估计你会觉得他脑子是不是有病。是的，他不仅有病，很可能是疯了。不同的是，埃隆不仅是疯了，他还拼上身家性命去实现。埃隆是一个具有颠覆性的创业者，每次创业都是行业史无前例的开端。 Zip2，是埃隆第一家创业的公司，类似于现在的大众点评，把商户的信息放到互联网上，帮他们打广告。这个在现在看起来并什么了不起，当那个年代可是互联网还没发展起来的年代，所有信息几乎都是从黄页上获取的。 X.com，世界第一家具有银行性质的网络公司，后来和Paypal并购改名为Paypal。那时没人相信把金钱这么敏感的东西放到互联网上是可靠的，但埃隆做到了，并推广到全世界。 SpaceX，太空探索技术公司,可能是唯一一创新性廉价太空领域公司。它一支火箭的造价是传统造价1/5，甚至在后面可回收实现后是1/10。当时埃隆把X.com卖掉的钱，几乎全部都投到这家公司去了，而且要知道，造火箭并不是一件容易的事情，特别是没有国家的支持下。但它凭着自己实现梦想的动力，排除万难地把公司办下来了。 特斯拉，这个大家都很熟悉了，简直对传统汽车行业的降维打击。 SolarCity，一家专门发展家用光伏发电项目的公司，清洁能源商用推动者。 拿埃隆的话说就是，我等不急别人去实现这些东西了，我要自己去做！不知道大家发现没有，SpaceX + 特斯拉 + SolarCity到底是个什么东西？明显就是 “老子要上火星！！”（说实在的，要是我有钱，也想买辆特斯拉，支持下埃隆上火星） 特斯拉的创始人？？可能大家都以为埃隆是特斯拉的创始人，遗憾的是，特斯拉真正的创始人并不是他，而是天才工程师——马丁·艾伯哈德，电动车的技术革新者，破局者。在埃隆还在构想用电容作为电动车能源的时候，他已经提出用锂电池（目前特斯拉的电池技术核心）。当初马丁到处找合适的投资人实现他锂电池的电动车方案的时候，找到了埃隆，所以埃隆仅仅是当初特斯拉公司的风险投资人。但是随着公司的波动、各种挫折，特斯拉经历了五年都无法推出一款能够面向市场的电动车，埃隆作为特斯拉最大的股东，把这位创始人踢出局，自己拿起了公司的权杖。直到现在，埃隆和马丁依然有非常深的矛盾。 失败？那仅仅是个过程别看现在埃隆的公司都很风光，当初都经历不少生死瞬间。SpaceX，花了埃隆几乎全部身家，还要变卖资产，到处借钱，几年的时间，终于创造出第一颗火箭，但没升到半空中就爆炸了。再次历时一年多造出的火箭，终于升到地轨，机身不稳定再次爆炸。不久后熟悉的爆炸再次出现，此时SpaceX的钱只够再造两次火箭了。在公司创办的10年后，第四次火箭终于成功发射，SpaceX在航空霸主的地位开始确立。 特斯拉同样经历了一段非常长的探索期，在公司创办的5年后，已经有眉目要推出一款可用的电动车Roadster。但它的造价成本高达20万美元，和预计的市场价8.5万美元相差不止一倍，根本没办法推出市场，核心员工开始离开，公司濒临倒闭。此时就是埃隆掌权的开始，也开始扭转特斯拉的命运。 埃隆曾经经历过生死，一次与妻子去南非度假，患上了一种严重的疟疾。只要晚一天治疗，估计没今天的特斯拉了。那次后，埃隆瘦了45磅，感叹到，休假真的会掉命的。]]></content>
      <categories>
        <category>感悟</category>
      </categories>
      <tags>
        <tag>Elon Musk</tag>
        <tag>Tesla</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客重构之路——Jekyll迁移到Hexo]]></title>
    <url>%2Fhexo-blog%2F</url>
    <content type="text"><![CDATA[某一天，在Google搜索 “mongodb 事务”，我的博客赫然出现在结果列表前10。为了更加鞭策自己更新博客，总结更多技术，决定重新构建一个新版博客，定时写文章。 为何要迁移到Hexo之前用的jekyll，对于主题的支持，大部分是对主题的颜色、样式的配置，很少基于插件的一键配置。经常地，需要改样式，添加插件，需要自己找到对应的前端代码，添加修改。这样对于后端开发人员来说，非常的不友好。毕竟后端开发人员志不在此，花费时间去写前端代码，才能拥有一个完善可观的博客。而一些Hexo的主题，对于一些常用的插件，模块的样式做了很好的抽象，很多时候都能做到开箱即用的效果。或许这其实是社区的效果吧。下面是Zack‘s Blog的配置。 Hexo主题选择博客选择的主题是Next，是目前最多人用的Hexo主题，几乎支持所有主流插件，稍微配置下就能用，不需要改模板。样式是Gemini。 功能更新favicon网上选一个适合自己的icon，然后丢到 https://realfavicongenerator.net/ 生成各种适配的favicon。 搜索优化 baidu push google search site map 文章/站点字数统计symbols count time。统计文章的字数和大概阅读的时间。 文章访问统计leancloud。静态博客比较常用的数据存储服务。 网站分析google analytics。比百度靠谱很多，麻烦点就是查看数据需要翻墙。 本地搜索功能algolia search。 文章分享功能needmoreshare2。比起百度分享等好看不要太多。 评论系统valine。国内主流要不关闭，要不就备案，甚是麻烦，该评论插件用的也是leanCloud的 后续还有配置了一些打赏的功能等。个人还是喜欢简约点的风格，所以很多花哨的东西也没用上。毕竟关注内容，多写博文才是主要的。]]></content>
      <categories>
        <category>工具分享</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker-开发人员必须要知道的Docker]]></title>
    <url>%2Fdeveloper-docker%2F</url>
    <content type="text"><![CDATA[随着微服务的发展，服务容器化已经是开发必须掌握的课题。很多开发以为，项目部署，集群管理与自己无关，那是运维的事，但如今技术百花齐放，不同语言的项目有不同的部署方式，部署java需要懂jvm、tomcat、mvn，部署node项目要懂npm、v8等等。假若一个集群有多种技术栈的项目，那么运维即使不精通，也要基本了解全部的知识，否则服务器出问题，也不知道从哪里入手，再加上各种杂七杂八的项目配置，运维还要知道配置对应的业务内容，到最终只能增加人手去管理真正的运维工作（网络、ECS、负载、集群等等）。所以，如何切分开发和运维的工作，是很重要的课题。docker是一个很好的切入点，开发负责编码开发，然后把服务级别的docker给打包好，交付给集群使用，运维管理docker集群。当然docker还有非常多的好处，一次打包，到处运行；配置好一个集群，可以随意开多个独立环境，对于产品复制和测试调试都非常好用。下面一个项目中打包成一个docker服务的简单方案。 docker基本操作首先肯定要熟悉docker的基本操作和原理。对于这部分内容，网上有非常多教程，在此不再赘述。提供个参考资料Docker — 从入门到实践 DockerfileDockerfile是用于构建镜像用的，指定某个镜像，执行一系列的操作，然后形成一个新的镜像。例如制作一个带node的centos镜像 FROM centos:centos7 MAINTAINER zack@xxxx.com RUN yum -y update; yum clean all RUN yum -y install epel-release; yum clean all RUN yum -y install nodejs npm; yum clean all 具体操作，参考上一节中的资料。 基础运行环境镜像构建docker的内部是一种层级结构，上层是依赖下层构建的。例如一个安装了node的ubuntu，上层是node，下层是ubuntu，整体加起来就是带着node环境的ubuntu。在镜像制作后上传中，如果在仓库里面，底层已经存在的话，只会上传上层，但如果中间一层更改了，即上层依赖的层变更了，中间那层开始都要重新制作上传。 b56f2cfbd344: Pushed 2c3396684cbe: Pushing[================&gt; ] 20.58MB/63.15MB 58eb630a9b0e: Layer already exists 69b918931b8c: Layer already exists 8a61997aeb56: Layer already exists 1a4a50f5502f: Layer already exists 3f5bd7889e12: Layer already exists 25baa3ba1903: Layer already exists 5b1e27e74327: Layer already exists 04a094fe844e: Layer already exists 所以，构建镜像也是门学问，要减少每次构建的执行时间，高效完成。一个原则就是尽量保证底层是不变动的。实践来看，很容易就能分出两层，运行环境为底层、项目服务。运行环境，项目所依赖的基础运行环境，例如jdk、tomcat、node、npm等等。这层一般不需要变动，构建一次即可，并且可以供同类项目使用。项目服务，业务层面可对外提供服务的层级。一般把打包好的war包、可执行的项目放这里，启动该镜像即可提供服务。 基础镜像一般不以ubunut或centos为基础构建，这些版本docker镜像至少要几百M，构建出来体积太大，而且很多功能不需要，而且很占空间和带宽。所以大多基础镜像的构建是基于alpine构建的，一般才几十M。apline最linux可运行最简单的系统，或后续需要一些系统工具例如curl、wget等都需要额外安装。 简单的镜像可以在docker-library 找到对应的官方Dockerfile，然后再此基础上修改。下面以构建java web项目为例，构建一个jdk、tomcat的基础环境镜像。 FROM openjdk:8-jre-alpine ENV CATALINA_HOME /usr/local/tomcat ENV PATH $CATALINA_HOME/bin:$PATH RUN mkdir -p &quot;$CATALINA_HOME&quot; WORKDIR $CATALINA_HOME # let &quot;Tomcat Native&quot; live somewhere isolated ENV TOMCAT_NATIVE_LIBDIR $CATALINA_HOME/native-jni-lib ENV LD_LIBRARY_PATH ${LD_LIBRARY_PATH:+$LD_LIBRARY_PATH:}$TOMCAT_NATIVE_LIBDIR RUN apk add --no-cache gnupg # see https://www.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/KEYS # see also &quot;update.sh&quot; (https://github.com/docker-library/tomcat/blob/master/update.sh) ENV GPG_KEYS 05AB33110949707C93A279E3D3EFE6B686867BA6 07E48665A34DCAFAE522E5E6266191C37C037D42 47309207D818FFD8DCD3F83F1931D684307A10A5 541FBE7D8F78B25E055DDEE13C370389288584E7 61B832AC2F1C5A90F0F9B00A1C506407564C17A3 713DA88BE50911535FE716F5208B0AB1D63011C7 79F7026C690BAA50B92CD8B66A3AD3F4F22C4FED 9BA44C2621385CB966EBA586F72C284D731FABEE A27677289986DB50844682F8ACB77FC2E86E29AC A9C5DF4D22E99998D9875A5110C01C5A2F6059E7 DCFD35E0BF8CA7344752DE8B6FB21E8933C60243 F3A04C595DB5B6A5F1ECA43E3B7BBB100D811BBE F7DA48BB64BCB84ECBA7EE6935CD23C10D498E23 RUN set -ex; \ for key in $GPG_KEYS; do \ gpg --keyserver ha.pool.sks-keyservers.net --recv-keys &quot;$key&quot;; \ done ENV TOMCAT_MAJOR 8 ENV TOMCAT_VERSION 8.0.48 ENV TOMCAT_SHA1 d2446c127c9b11f88def11e542af98998071d91d ENV TOMCAT_TGZ_URLS \ # https://issues.apache.org/jira/browse/INFRA-8753?focusedCommentId=14735394#comment-14735394 https://www.apache.org/dyn/closer.cgi?action=download&amp;filename=tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz \ # if the version is outdated, we might have to pull from the dist/archive :/ https://www-us.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz \ https://www.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz \ https://archive.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz ENV TOMCAT_ASC_URLS \ https://www.apache.org/dyn/closer.cgi?action=download&amp;filename=tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz.asc \ # not all the mirrors actually carry the .asc files :&apos;( https://www-us.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz.asc \ https://www.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz.asc \ https://archive.apache.org/dist/tomcat/tomcat-$TOMCAT_MAJOR/v$TOMCAT_VERSION/bin/apache-tomcat-$TOMCAT_VERSION.tar.gz.asc RUN set -eux; \ \ apk add --no-cache --virtual .fetch-deps \ ca-certificates \ openssl \ ; \ \ success=; \ for url in $TOMCAT_TGZ_URLS; do \ if wget -O tomcat.tar.gz &quot;$url&quot;; then \ success=1; \ break; \ fi; \ done; \ [ -n &quot;$success&quot; ]; \ \ echo &quot;$TOMCAT_SHA1 *tomcat.tar.gz&quot; | sha1sum -c -; \ \ success=; \ for url in $TOMCAT_ASC_URLS; do \ if wget -O tomcat.tar.gz.asc &quot;$url&quot;; then \ success=1; \ break; \ fi; \ done; \ [ -n &quot;$success&quot; ]; \ \ gpg --batch --verify tomcat.tar.gz.asc tomcat.tar.gz; \ tar -xvf tomcat.tar.gz --strip-components=1; \ rm bin/*.bat; \ rm tomcat.tar.gz*; \ \ nativeBuildDir=&quot;$(mktemp -d)&quot;; \ tar -xvf bin/tomcat-native.tar.gz -C &quot;$nativeBuildDir&quot; --strip-components=1; \ apk add --no-cache --virtual .native-build-deps \ apr-dev \ coreutils \ dpkg-dev dpkg \ gcc \ libc-dev \ make \ &quot;openjdk${JAVA_VERSION%%[-~bu]*}&quot;=&quot;$JAVA_ALPINE_VERSION&quot; \ openssl-dev \ ; \ ( \ export CATALINA_HOME=&quot;$PWD&quot;; \ cd &quot;$nativeBuildDir/native&quot;; \ gnuArch=&quot;$(dpkg-architecture --query DEB_BUILD_GNU_TYPE)&quot;; \ ./configure \ --build=&quot;$gnuArch&quot; \ --libdir=&quot;$TOMCAT_NATIVE_LIBDIR&quot; \ --prefix=&quot;$CATALINA_HOME&quot; \ --with-apr=&quot;$(which apr-1-config)&quot; \ --with-java-home=&quot;$(docker-java-home)&quot; \ --with-ssl=yes; \ make -j &quot;$(nproc)&quot;; \ make install; \ ); \ runDeps=&quot;$( \ scanelf --needed --nobanner --format &apos;%n#p&apos; --recursive &quot;$TOMCAT_NATIVE_LIBDIR&quot; \ | tr &apos;,&apos; &apos;\n&apos; \ | sort -u \ | awk &apos;system(&quot;[ -e /usr/local/lib/&quot; $1 &quot; ]&quot;) == 0 { next } { print &quot;so:&quot; $1 }&apos; \ )&quot;; \ apk add --virtual .tomcat-native-rundeps $runDeps; \ apk del .fetch-deps .native-build-deps; \ rm -rf &quot;$nativeBuildDir&quot;; \ rm bin/tomcat-native.tar.gz; \ \ # sh removes env vars it doesn&apos;t support (ones with periods) # https://github.com/docker-library/tomcat/issues/77 apk add --no-cache bash; \ find ./bin/ -name &apos;*.sh&apos; -exec sed -ri &apos;s|^#!/bin/sh$|#!/usr/bin/env bash|&apos; &apos;{}&apos; + # verify Tomcat Native is working properly RUN set -e \ &amp;&amp; nativeLines=&quot;$(catalina.sh configtest 2&gt;&amp;1)&quot; \ &amp;&amp; nativeLines=&quot;$(echo &quot;$nativeLines&quot; | grep &apos;Apache Tomcat Native&apos;)&quot; \ &amp;&amp; nativeLines=&quot;$(echo &quot;$nativeLines&quot; | sort -u)&quot; \ &amp;&amp; if ! echo &quot;$nativeLines&quot; | grep &apos;INFO: Loaded APR based Apache Tomcat Native library&apos; &gt;&amp;2; then \ echo &gt;&amp;2 &quot;$nativeLines&quot;; \ exit 1; \ fi # install maven #RUN wget http://apache-mirror.rbc.ru/pub/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz #RUN tar xzvf apache-maven-3.3.9-bin.tar.gz #RUN cp -R apache-maven-3.3.9 /usr/local/bin #RUN export PATH=apache-maven-3.3.9/bin:$PATH #RUN export PATH=/usr/local/bin/apache-maven-3.3.9/bin:$PATH #RUN ln -s /usr/local/bin/apache-maven-3.3.9/bin/mvn /usr/local/bin/mvn #RUN ls -l /usr/local/bin #RUN echo $PATH #EXPOSE 8080 #CMD [&quot;catalina.sh&quot;, &quot;run&quot;] 这里面不构建maven环境。因为maven是用于项目的构建，而不是用于执行的。项目的构建应该是构建的机器上做，以减少docker的打包成本。同时也不需要在基础镜像中指定暴露端口和启动命令，这部分应在项目镜像构建里面做。 项目执行镜像构建有了基础的镜像，在此层做的东西就非常简单了，只要把项目运行的东西往docker里面塞，让docker里面的服务跑起来就行了。例如java web项目，需要将war包放到docker里面的的tomcat目录，暴露对应端口，指定镜像启动命令。 FROM registry.xxxx/xxxx:latest ENV CATALINA_HOME /usr/local/tomcat ENV MODEL xxx #ENV APP /app WORKDIR $CATALINA_HOME/webapps COPY ${MODEL}/target/*war . EXPOSE 8080 CMD [&quot;catalina.sh&quot;, &quot;run&quot;] 到此镜像可执行的镜像已经构建完了。细心的同学可能发现了，到目前为止都没有配置过项目的启动参数。是的，镜像本身不应该包含项目的环境变量参数等，它应该在容器启动时候指定输入。否则每次变更配置需要重新构建，重新打包上传，不利于镜像扩展使用。在后面会介绍在启动时添加环境变量的方案。 Make批处理构建一个镜像，经常会包含多个命令，并且很多时候需要切换执行目录或复制移动文件。所以日常开发用make来配合镜像的构建上传下载打tag是非常有必要的，特别是要制作不同产品，不同环境的镜像。 base: echo building ${NAME}-base:${TAG} cp ${model}/docker/base/Dockerfile . docker build -t ${REGISTRY}/${NAME}-base:${TAG} . docker tag ${REGISTRY}/${NAME}-base:${TAG} ${REGISTRY}/${NAME}-base:latest rm Dockerfile docker push ${REGISTRY}/${NAME}-base:${TAG} docker push ${REGISTRY}/${NAME}-base:latest war: # TODO 加MD5校验，代码没改，不用执行 echo package war ${model} mvn -pl ${model} -am -Dmaven.test.skip=true -Denv=xxx install local:war echo building ${NAME}:${TAG} cp ${model}/docker/local/Dockerfile . docker build -t ${REGISTRY}/${NAME}-${model}:latest . #docker build -t ${REGISTRY}/${NAME}-${model}:${TAG} . #docker tag ${REGISTRY}/${NAME}-${model}:${TAG} ${REGISTRY}/${NAME}-${model}:${FIXTAG} rm Dockerfile #docker push ${REGISTRY}/${NAME}-${model}:${TAG} #docker push ${REGISTRY}/${NAME}-${model}:${FIXTAG} docker push ${REGISTRY}/${NAME}-${model}:latest 一键拉起一套栈很多时候，一个web项目需要很多依赖的服务，例如db，redis，zookeeper等等。如果想一套拉起多个容器服务，可以用docker-compose。很多人搞不清Dockerfile和docker-compose的区别，前者是构建镜像用的，一个是创建容器，即就是镜像与容器的关系，所以到底什么操作在Dockerfile里面做，什么操作在compose里面做，搞清楚这个关系就很容易区分了。很多集群管理工具，像rancher、k8s都支持用docker-compose导出一套栈进来管理。 以下是一个mysql、zookeeper、memcached及web服务自身的例子。执行docker-compose up即可一套拉起。 version: &quot;2&quot; services: mysql: image:mysql zookeeper: image: zookeeper memcached: image: memcached app: image: registry.xxxx/xxxxx:latest environment: - JAVA_OPTS=-server -Xms512m -Xmx512m -Xss256K -Duser.timezone=GMT+08 ports: - &quot;8080:8080&quot; depends_on: - memcached - zookeeper - mysql links: - memcached - zookeeper - mysql]]></content>
      <categories>
        <category>工具分享</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongo Aggregation Result Exceeds Maximum Document Size解决方法]]></title>
    <url>%2FMongo-exceed-size%2F</url>
    <content type="text"><![CDATA[当用aggregate处理大量数据超出大小限制时，可以用以下方法解决。 Mongo的限制 结果大小限制 对于聚合操作的结果返回，MongoDB是对其有大小限制的。从2.6版本开始，MongoDB对大小的限制为16M。 内存大小限制 对于聚合操作过程，MongoDB的对内存限制是100M。一般来说，超过这个限制就会产生一个错误。但如果想超过这个限制则需要在options参数内加上 allowDiskUse: true 解决方法针对上面 2. 的解决方法以给出。对于 1. 有以下方法： a. match完先project，去掉不需要的字段，减少返回值大小b. 用 MongoDB 提供的GridFS API 存储数据的大小 参考 : https://docs.mongodb.com/manual/core/aggregation-pipeline-limits/ aggregation-pipeline-limits MongoDBhttps://docs.mongodb.com/manual/reference/limits/#BSON-Document-Size BSON-Document-Size MongoDB]]></content>
      <categories>
        <category>踩坑记录</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ES6-Number.EPSILON 用法]]></title>
    <url>%2Fnumber-epsilon%2F</url>
    <content type="text"><![CDATA[用 ES6 中，Number的常量扩展 ，以校验javascript的浮点数运算问题 ES6在Number对象上面，新增一个极小的常量 Number.EPSILON。 Number.EPSILON // 2.220446049250313e-16 Number.EPSILON.toFixed(20) // &apos;0.00000000000000022204&apos; 引入一个这么小的量的目的，在于为浮点数计算，设置一个误差范围。我们知道浮点数计算是不精确的。 0.1 + 0.2 // 0.30000000000000004 0.1 + 0.2 - 0.3 // 5.551115123125783e-17 5.551115123125783e-17.toFixed(20) // &apos;0.00000000000000005551&apos; 但是如果这个误差能够小于 Number.EPSILON ，我们就可以认为得到了正确结果。 5.551115123125783e-17 &lt; Number.EPSILON // true 因此，Number.EPSILON的实质是一个可以接受的误差范围。 function withinErrorMargin (left, right) { return Math.abs(left - right) &lt; Number.EPSILON; } withinErrorMargin(0.1 + 0.2, 0.3) // true withinErrorMargin(0.2 + 0.2, 0.3) // false 上面的代码为浮点数运算，部署了一个误差检查函数。]]></content>
      <categories>
        <category>JavaScript</category>
      </categories>
      <tags>
        <tag>ES6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB更新数组中某个对象的元素]]></title>
    <url>%2Fmongo-update-array%2F</url>
    <content type="text"><![CDATA[经常一些场景当中，需要更新数组中某个对象的元素。用$操作符即可完成这操作。可以把$理解为数组下标 例子一个包含数组sign表文档 { user_id:&apos;abc&apos; signDate:[ { dateTime:&apos;2016-8-15&apos; isSign:1 }, { dateTime:&apos;2016-8-16&apos; isSign:0 } ] } 现在想把8-16那记录的isSign改成1。除了把整个数组拿出来修改再整个update回去外。就是用$去更新，搭配$elemMatch db.sign.update( { user_id: &apos;abc&apos; , signDate:{$elemMatch:{dateTime:&apos;2016-8-16&apos;}} }, //查找对应记录，精确到哪些数组 {$set: {&apos;signDate.$.isSign&apos;:1}}) //更新找到的记录,$代表获取的下标 这个方法可以更新多条记录，不过最后要加上{multi:true}。 如果数组不是一个对象，可以不用$elemMatch。而是直接当成一个属性去查找。 { user_id: &apos;aaa&apos; score:[90,88] } 把上面记录score成绩小于90的记录改为100 db.score.update( {user:id:&apos;aaa&apos; , score:{$lt:90}}, //直接查也行, sscore:88 {$set:{&apos;score.$&apos;:100}} )]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七牛静态资源存储]]></title>
    <url>%2Fpicture-qiniu%2F</url>
    <content type="text"><![CDATA[用静态博客已经有了一段时间。经常会修改博客的界面，更换图片；写博文会用一些图片。但是在加载这些图片的时候，由于图片大，而且是从github上读取（网络传输速率慢），导致在浏览博客的时候会特别不友好，一些文字加载出来了，但是一些样式和图片并没有加载出来。所以决定首先把加载最慢的图片放到七牛上面，大大加快加载速度。 为什么要使用七牛存储 数据的在线托管七牛采用全分布式系统架构以及存储技术，主要存储图片、音视频等静态文件，并对数据实行多机房互备和跨IDC修复，从而保障数据存储的安全性。 数据的传输加速七牛支持上传/下载双向加速，对于单个文件的上传没有大小限制，并且支持断点续传。七牛在全国部署了500多个加速节点，用户可以选择任意的IDC就近上传下载。 云端数据处理七牛提供丰富的图片处理服务，例如缩略图、图文混排、水印、自定义裁剪区域、防盗链，原图保护等。七牛还支持常见的ffmpeg音视频格式转换，视频取帧以及流媒体传输协议（HLS）。 免费提供了免费CDN配额：存储空间 10GB，每月下载流量 10GB，每月 PUT/DELETE 10万次请求，每月 GET 100万次请求。月流量在10GB以下的博客基本上可以一直免费使用七牛云存储CDN服务了。所以像本站这类的静态博客，非常适合使用七牛作为静态资源的云存储 方便数据移植在使用七牛之前，本站所有的图片都是放在项目源码当中的。因为项目放在github中，所以所有的图片有对应的外链，但是一旦项目转移到其他地方，或者更换项目，图片的外链就会对应改变，非常不便于移植。存储在七牛的话，图片的外链一直都是那一个，就不会有这种麻烦。 图片转移登陆七牛的网站，注册一个账号，新建一个存储空间即可以上传图片，获取外链，在博客中引用。这张图片链接就是七牛的外链，可以右键查看源看看。 七牛图片处理APi七牛还提供很多关于图片处理的api。 图片裁剪 分辨率调整 图片格式转换 如果你觉得图片加载速度还是慢，在不影响体验下，可以降到分辨率，转换图片格式为webp。原图 降低分辨率的图 根据原图转换webp格式（一些浏览器不支持） 具体加载速度可以按f12看效果。 具体api文档七牛图片处理api]]></content>
      <categories>
        <category>工具分享</category>
      </categories>
      <tags>
        <tag>七牛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB之$unwind操作符]]></title>
    <url>%2Fmongodb-unwind%2F</url>
    <content type="text"><![CDATA[在aggregate中，常常会遇到一些字段属性是数组对象，然后又需要对这些数组对象进行统计。这时候就需要用到$unwind操作符。这是一个常用的，又容易被忽略的一个操作。 定义 field 版 { $unwind: &lt;field path&gt; } document版 { $unwind: { path: &lt;field path&gt;, includeArrayIndex: &lt;string&gt;, preserveNullAndEmptyArrays: &lt;boolean&gt; } } \ 你要打散的字段 includeArrayIndex，分配一个存该数组索引的字段 preserveNullAndEmptyArrays，是否输出空内容。 场景一个用户表user，其中一个字段是一个数组对象，存的是用户的奖励信息。这时需要统计用户A所有奖励类型为b的总额。 { user_id:A_id , bonus:[ { type:a ,amount:1000 }, { type:b ,amount:2000 }, { type:b ,amount:3000 } ] } unwind操作： db.user.aggregate([ {$unwind:bonus} ]) //结果 {user_id : A_id , bonus:{type : a ,amount : 1000}} {user_id : A_id , bonus:{type : b ,amount : 2000}} {user_id : A_id , bonus:{type : b ,amount : 3000}} 统计： db.user.aggregate([ {$match: {user_id : A_id} }, {$unwind:bonus}, {$match: {&apos;bonus.type&apos; : b} }, {$group: {_id : &apos;$user_id&apos; , amount : {$sum : {&apos;$bonus.amount&apos;}} }} ]) //结果 {_id:A_id , amount : 5000} 参考 : https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/ $unwind MongoDB]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git基础之tag功能]]></title>
    <url>%2Fgit-tag%2F</url>
    <content type="text"><![CDATA[git的基础功能中有个打标签的功能。这功能其实非常有用，往往在线上发布时会用到。 tag作用像其他版本控制系统（VCS）一样，Git 可以给历史中的某一个提交打上标签，以示重要。 比较有代表性的是人们会使用这个功能来标记发布结点（v1.0 等等）。当你在线上发布最新的代码时，最好每次都打上一个tag，因为当这次发布的代码失败，有bug的时候，你可以快速checkout上次的稳定tag，以实现代码的快速回滚，待你修复好新代码后，再重新发布，打上一个稳定的标记tag。另外在一些场景也会用到打tag来实现持久化构建的功能。 列出标签$ git tag v0.1 v1.3 列出特定的标签： $ git tag -l &apos;v1.8.5*&apos; v1.8.5 v1.8.5-rc0 v1.8.5-rc1 v1.8.5-rc2 v1.8.5-rc3 v1.8.5.1 v1.8.5.2 v1.8.5.3 v1.8.5.4 v1.8.5.5 创建标签 轻量标签（lightweight），无标记信息： $ git tag v1.4-lw $ git tag v0.1 v1.3 v1.4 v1.4-lw v1.5 $ git show v1.4-lw commit ca82a6dff817ec66f44342007202690a93763949 Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number 附注标签（annotated）。 $ git tag -a v1.4 -m &apos;my version 1.4&apos; $ git tag v0.1 v1.3 v1.4 $ git show v1.4 tag v1.4 Tagger: Ben Straub &lt;ben@straub.cc&gt; Date: Sat May 3 20:19:12 2014 -0700 my version 1.4 commit ca82a6dff817ec66f44342007202690a93763949 Author: Scott Chacon &lt;schacon@gee-mail.com&gt; Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number 提交标签默认情况下，git push 命令并不会传送标签到远程仓库服务器上。 在创建完标签后你必须显式地推送标签到共享服务器上。 这个过程就像共享远程分支一样 - 你可以运行 git push origin [tagname]。 //提交单个tag $ git push origin v1.5 //提交全部tag $ git push origin --tags checkout标签在 Git 中你并不能真的检出一个标签，因为它们并不能像分支一样来回移动。 如果你想要工作目录与仓库中特定的标签版本完全一样，可以使用 git checkout -b [branchname] [tagname] 在特定的标签上创建一个新分支： $ git checkout -b version2 v2.0.0 Switched to a new branch &apos;version2&apos; 在线上执行回滚的时候，用一个可用稳定的tag，创建一个新的临时分支，让该分支在线上先跑。然后再去修复master的代码，等待修复完，再重新把线上分支切回master，以实现快速代码回滚。 参考 : https://git-scm.com/book/en/v2/Git-Basics-Tagging Git Basics - Tagging]]></content>
      <categories>
        <category>其他技术</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb拥有事务的解决方案（二）]]></title>
    <url>%2Fmongo-transaction2%2F</url>
    <content type="text"><![CDATA[mongodb拥有事务的解决方案（一） 接上篇博文，本文将探索下mongodb的二段提交方案去解决事务问题。 背景 在MongoDB中，操作单个文档（document）是原子性的；但是，涉及到多个文档的操作，也就是常说的“多文档事务”，是非原子性的。由于document可以设计的非常复杂，包含多个“内嵌的”文档，因此单个文档的原子性为很多实际场景提供了必要的支持。 尽管单文档原子操作很强大，但在很多场景下依然需要多文档事务。当执行一个由几个顺序操作组成的事务时，可能会出现某些问题，例如： 原子性：如果某个操作失败了，在同一个事务中前面的操作回滚到最初的状态（即，要么全做，要么全部做）。 一致性：如果发生了严重故障将事务中断（比如：网络、硬件故障），数据库必须能够恢复到一致的状态。 在需要多文档事务的场景中，你可以实现两阶段提交来完成场景需求。两阶段提交可以保证数据的一致性，如果发生错误，可以恢复到事务开始之前的状态。在事务执行过程中，无论发生什么情况都可以还原到数据和状态的准备阶段。 自官方文档 二段的含义两段指prepare阶段和commit阶段：Prepare：TM(transaction manager)给每个参与者（resource manager）发送prepare信息。每个参与者要么直接返回失败，要么在本地执行事务（记录日志和rollback的信息），但不commit。Commit：如果TM收到了任一参与者的失败消息或超时，那么TM会发rollback给其他的参与者，参与者会执行rollback并在最后释放锁资源。否则则发送commit让所有参与者完成事务。 这样能保证在事务提交前尽可能完成所有能完成的工作，最后的commit是一个耗时很短的操作，错误概率相对很低。相比单一阶段的commit，两段式更加可靠但是会消耗更多时间，所以会提高锁资源的冲突，加大了死锁的发生几率。 MongoDB中的两段式提交以下来自官方文档中的典型例子。从A账户资金转移到B账户。MongoDB中的一个accounts集合保存了账户的name和余额balance信息，以及当前事务的数组pendingTransactions。 首先创建两个账户： db.accounts.save({name: &quot;A&quot;, balance: 1000, pendingTransactions: []}) db.accounts.save({name: &quot;B&quot;, balance: 1000, pendingTransactions: []}) 然后创建事务集合transactions，来保存事务的信息——源账户source、目标账户dest、转移金额value、事务状态state。如果通过账户A转移一笔钱到账户B时，将通过下面的方式发起一个事务（state为initial） db.transactions.save({source: &quot;A&quot;, destination: &quot;B&quot;, value: 100, state: &quot;initial&quot;}) 找到transactions集合中对应initial状态文档，并改为pending状态 t = db.transactions.findOne({state: &quot;initial&quot;}) db.transactions.update({_id: t._id, state: &quot;initial&quot;}, {$set: {state: &quot;pending&quot;}}) 为每一个账户应用事务，并记录事务的_id。 db.accounts.update( { name: t.source, pendingTransactions: { $ne: t._id } }, { $inc: { balance: -t.value }, $push: { pendingTransactions: t._id } }) db.accounts.update( { name: t.destination, pendingTransactions: { $ne: t._id } }, { $inc: { balance: t.value }, $push: { pendingTransactions: t._id } }) 检查账户状态，是否应用成功 将事务文档状态改为commited db.transactions.update({_id: t._id}, {$set: {state: &quot;committed&quot;}}) 然后更新accounts集合，相当于一个释放锁的过程。 db.accounts.update({name: t.source}, {$pull: {pendingTransactions: t._id}}) db.accounts.update({name: t.destination}, {$pull: {pendingTransactions: t._id}}) 将事务状态设置为done。 db.transactions.update({_id: t._id}, {$set: {state: &quot;done&quot;}}) 深入理解对应二段的含义，该例子的 2.对应就是二段含义图中的预备。该过程会记录事务发生的对象，及将要发生的变化内容。在这操作当中，可以尽可能把信息存储起来，，以方便事务失败的回滚 3.4.5.对应就是二段图中的就绪。即尝试完成该事务，把结果返回给事务管理器。 6.对应则是提交。确认事务的结果。 7.对应是已提交。告知事务管理器，把事务close掉。 再结合上篇博文讲到$isolated操作符，应该把以上的所有update操作的都加上，以避免多请求的并发操作。 事务回滚当出现一些不可恢复的错误时（例如其中一个文档不存在、文档条件(balance)不满足等），就需要进行回滚操作。有两种可能的回滚方法： 完成设置事务状态为done后，你需要完全提交这个事务，而不能进行回滚操作。可以创建一个新的事务来转换源和目标的文档。 完成设置事务状态为pending和d.将事务状态设置为commited之间，你需要执行下面的步骤： //设置事务状态为canceling db.transactions.update({_id: t._id, state: &quot;pending&quot;}, {$set: {state: &quot;canceling&quot;}}) //回滚事务：执行一系列的相反的操作。 db.accounts.update({name: t.source, pendingTransactions: t._id}, {$inc: {balance: t.value}, $pull: {pendingTransactions: t._id}}) db.accounts.update({name: t.destination, pendingTransactions: t._id}, {$inc: {balance: -t.value}, $pull: {pendingTransactions: t._id}}) db.accounts.find() //设置事务状态为canceled. db.transactions.update({_id: t._id}, {$set: {state: &quot;canceled&quot;}}) 参考 : https://docs.mongodb.com/manual/tutorial/perform-two-phase-commits/ mongodb官方二段提交官方文档]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mongodb拥有事务的解决方案（一）]]></title>
    <url>%2Fmongo-transaction%2F</url>
    <content type="text"><![CDATA[mongodb拥有事务的解决方案（二） 最近因为node的并发优点加上mongodb的无事务性被坑了几把，所以找了一些解决方案。 Redis内存锁（不可靠） 用mongodb自带的原子操作 （使用范围局限） 二段提交 发生场景 想增加一条记录，当这记录不存在的时候。在代码层面操作，先find查找判断是否存在记录,没有则insert一条新记录。但并发同时对一个记录操作时，如find操作读取并发了，由于还没有进行insert操作，导致find都没有找到记录。最终导致插入了两条记录。 并发查询更新同一条记录。并发查询获取信息操作，导致更新操作数据不正确。例如某账户余额是10元，在并发查询时， 同时获得10元信息。然后各提取2元，但是由于查询到的余额是10，所以更新操作无论是并发多少，结果都是8。 Redis内存锁这是当时一种临时解决方案。对于上述场景1、2，只要用记录的唯一标识在redis中增加一个内存锁即可。例如我想要为A用户注册新增一条用户记录，当且仅当他之前没有注册过（数据库中没有记录），我们可以用用户的身份证号（唯一），作为键先写入redis当中加锁（node中有redlock等npm包），然后再插入操作，操作完成后解锁即可。 Thenjs(function(cont){ redlock.lock(key,10,(err,lock) =&gt;{ lockobj = lock cont(err) }) }) .then(function(cont){ update/insert..... }) .then(function(cont){ redlock.unlock(lockboj,(err,result)=&gt;{ cont() }) }) .fail(function（cont){ console.log(&apos;err&apos;) }) 但是这种方法没有根本解决问题，因为这组合的操作也不是原子性的。假如第一个进来的锁住，然后insert操作因为数据库问题卡住，一直没有回调，时长超过了redis锁设置的时间，同样会让第二请求进入insert操作。如果说把redis缓存时间设置一个超长时间，例如1天，那么当在update或insert操作时，程序挂了。锁就解不了了，导致该用户在缓存时间内无法注册，除非手工清理。所以redis锁只能减轻mongodb + node并发无事务的状况，而不能根本解决问题。 mongodb自带的原子操作虽然mongodb没有事务处理，但是它自带一些原子性的复合操作。在某些特定场景下使用能达到事务的效果，当然缺陷就是只能是特定的某些场景。总结就是，能用原子操作就用原子操作。 db.module.findAndModify() 先查找后更新操作 例如一个图书管理系统，查一本书是否可以借阅。那么首先需要找到这本书，然后判断书的借阅状态，若未必借阅，则把状态设置为已借阅， 把书借阅出去。使用这个函数就可以达到先查找后更新的原子操作。 db.book.findAndModify({ query:{ id: bookid, status: &apos;free&apos;}, update:{ $set:{ status: &apos;busy&apos; } } }) 其他操作符:$unset (删除一个field)$inc (对文档的某个值为数字型的field进行增减的操作)$push (把value追加到field里面去，field一定要是数组类型才行，如果field不存在，会新增一个数组类型加进去)$pushAll (同$push,只是一次可以追加多个值到一个数组字段内)$pull (从数组field内删除一个等于value值)$addToSet (增加一个值到数组内，而且只有当这个值不在数组内才增加)$pop (删除数组的第一个或最后一个元素)$rename (修改字段名称)$bit (位操作，integer类型) update with upsert 更新插入(upsert = update &amp; insert) 对应上述场景一，比较适合用这种方法。更新一条记录，若记录不存在则创建。例如 User.update({id : user_id }, param , { upsert : true }) 上面的例子，若查找不到id为user_id的文档，则插入数据为param的文档。这样可避免先查找后插入的并发问题。 update/remove with $isolated 文档隔离 Prevents a write operation that affects multiple documents from yielding to other reads or writes once the first document is written. By using the $isolated option, you can ensure that no client sees the changes until the operation completes or errors out. This behavior can significantly affect the concurrency of the system as the operation holds the write lock much longer than normal for storage engines that take a write lock (e.g. MMAPv1), or for document-level locking storage engine that normally do not take a write lock (e.g. WiredTiger), $isolated operator will make WiredTiger single-threaded for the duration of the operation. 总结的说就是在update/remove操作的时候，把选中的文档进行加锁，直到操作完成或错误退出。在这加锁的过程中，不允许其他操作读取和写入这些文档。 db.foo.update( { status : &quot;A&quot; , $isolated : 1 }, { $inc : { count : 1 } }, { multi: true } ) 在上面这个例子当中，如果没有$isolated，由于是多multi操作，若有其他操作对这些文档读取写入，会破坏文档的正确数据。]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>mongodb</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Jekyll和github Pages博客搭建全过程(二)]]></title>
    <url>%2Fbuilt-blog-2%2F</url>
    <content type="text"><![CDATA[基于jekyll和github pages博客搭建全过程(一) 基于jekyll和github pages博客搭建全过程(三) 上一篇文章说到了博客的基本搭建和编写。如果对个人博客要求不高的同学其实就已经足够了。本文主要是介绍如何更加深入定制个人博客。 本地安装jekyll如果已经完成上文的博客搭建，相信你已经对jekyll的项目结构有一定的了解。若你想更好的定制自己的blog，那么肯定需要自己修改项目的源码，改它的样式界面。但是每次修改代码要上传到github，等待编译刷新页面才能看到效果，那效率就太慢了。所以想要高效地修改维护自己的blog，免不了要本地安装jekyll。以下是linux系统的安装步骤 ruby安装ruby安装是个门槛，因为新版的jekyll需要2.0以上的ruby，但是你直接用apt-get install命令安装，最多只会安装到1.9版本。但是有两个巧妙的解决方法 直接安装ruby2.0 $ sudo apt-get install ruby2.0，然后用关联命令关联为ruby 直接用命令$ sudo apt-get install ruby安装1.9的ruby，待会安装jekyll，不要安装最新版，安装2.5.0版本 用RVM去安装指定版本的ruby，比较麻烦，参考 https://ruby-china.org/wiki/install_ruby_guide rubygems安装rubygems是ruby的包管理器，就像node的npm，如果你按上面步骤安装到这一步，一般都会失败，因为rubygems还依赖ruby-dev，所以在安装rubygems前，需要执行$ sudo apt-get install ruby-dev jekyll安装这一步总的来说就是执行$ sudo gem install jekyll，但是一般都会有比较多的问题，例如缺少gcc，ruby版本太低等等，具体问题有太多，只能上google搜索下解决方法。 jekyll包依赖解决如果你的jekyll项目是从fork别人的代码，那么一般的都会引用到其他的包，在你启动项目时会报错，因为你还没安装。对应缺少的包，你需要用gem去安装，$ sudo gem install jekyll-sitemap。你可以在项目中的Gemfile查看你引用的包，并主动安装它。 jekyll服务启动cd到对应的项目目录，输入$sudo jekyll serve就可以启动项目了。对于你的代码，只要不改到_config.yml配置文件，jekyll会实时编译你的改动，你每改动一次只要即使刷新页面就可以看到了。 Windows环境怎么办？我建议，如果是Windows的话，就别折腾去本地安装jekyll了。因为linux环境都已经够复杂的了。若你还是希望在Windows下安装jekyll，你只能寄于docker或者虚拟机了。否则你还是乖乖的上传代码看效果吧。 定制自己的项目这一节大概粗略介绍以下，如何有技巧的、快速的定制自己的博客。 了解自己的模板一般来说我们的blog都是从网上拿的模板。在拿的时候，不要看完界面好看就fork。 你要先看看项目的REAME.md，看看它说明的功能是否符合你的需求，例如有没有tag统计功能，文章分类功能等。 看项目的_config.yml，这是整个项目的核心。优秀的jekyll项目，都会把功能配置、主要参数写到这个文件内，所以你看这文件，就基本知道有哪些功能，并对应修改它 修改前端很经常你在用一个新的模板时，会觉得某些地方不合适，例如有些facebook、twitter分享按钮不想要，想换成自己的一些联系方式。你会想删除或者调整一些内容，但又不知道去哪里修改这些东西。其实你或许不需要知道太多前端的知识，你只需要对整个项目全局搜索facebook，找出对应位置，在附近添加修改即可。没必要真的完全理解整个项目是如何编译运行的。 jekyll的深入一般jekyll的文件目录结构如下jekyll的启动运行大致如下流程： 读取_config.yml配置，修改到这个文件需要重启jekyll服务 读取根目录的index.html文件，但这文件一般没内容，需要递归读取 带_符号的目录都是最底层的生成文件，是项目启动后不能直接访问的，它们会编译填充进上级的目录文件中，这些目录都是不带_符号。 需要读取一些内容时，这些内容就会执行步骤3.，归并到上级可显示等级中呈现出来，而不是一次全加载。 大多数一个不带_符号的文件夹，（除了assets文件夹，存放可以直接访问的图片、css、js等）,是一个页面，然后由_layouts里的各个页面文件填充。 _includes文件一般是通用组件的页面 由此可见，一般你需要修改的文件大多是带_符号的文件，例如页面的修改，博客的编写等。而修改页面的title则是修改不带_符号的文件，因为这是建立页面的基础属性。 参考 : https://ruby-china.org/ ruby China主页 http://jekyllcn.com/ jekyll中文主页]]></content>
      <categories>
        <category>工具分享</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>jekyll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Jekyll和github Pages博客搭建全过程(三)]]></title>
    <url>%2Fbuilt-blog-3%2F</url>
    <content type="text"><![CDATA[基于jekyll和github pages博客搭建全过程(一) 基于jekyll和github pages博客搭建全过程(二) 本篇博文将讲如何为自己的jekyll博客配上一个炫酷的域名。 域名的购买域名购买推荐去 Godaddy，原因： 支持支付宝、银联 世界最大的域名注册商，信誉、安全、有保障 在Godaddy购买域名价格低廉，不过前提是你要去找到对应的优惠券，有些优惠券是用银联支付宝，有些是只能用visa、master、美国运通信用卡。不用优惠券，和万网、DNSPOD是一样价。 注册域名过户方便，用于域名交易等。 注册域名解析生效快，其他网站域名解析可能要一两天，Godaddy只要几分钟就行了。 至于购买的教程，网上大把，优惠券也大把，就没必要在这里赘叙了。 Godaddy域名配置在购买玩域名后，我们把域名的解析交给DNSPOD，而不是直接给Godaddy，因为它是免费，而且是国内的，速度会比较快。在Godaddy购买域名后，登陆后台的域名管理。一般购买完需要验证下个人的邮箱。然后点击管理DNS， 找到域名服务器这一栏，更改为自定义，然后输入 f1g1ns2.dnspod.net f1g1ns1.dnspod.net 到此以后，Godaddy的网站在你域名年限内，都可以不用管了，不过记得找东西把Godaddy的账号密码记下来，不然几年后就忘记了，续费不了。 DNSPOD域名解析为什么要对域名进行解析我们买到域名仅仅是买到了个地址的名字，但是这个地址放在哪里，我们还没有安排好。例如我们买了北京路这名字的使用权，但是北京路这名字用在哪条具体的路上，还不清楚。所以这一节将解决这个问题，为域名铺路。 首先我们在 DNSPOD 上注册一个账号，用QQ也行。 然后点解域名解析 按添加域名，把域名添加到dnspod中。 然后点击进入添加两条记录。一条是@记录，代表没有域名前缀www，另外一条则是www。就是说你设置以后，访问这个域名，带不带www都行。记录类型为CNAME，代表的是指向另外一个域名，具体解析交由给另外一个域名处理。因为我们已经有域名username.github.io，所以我们选择CNAME类型。如果是一个ip，则选择A类型 到此的DNSPOD域名解析就完成了 项目处理域名这一步是可选，但是又很重要。假如你完成上面的步骤，你购买的域名只是做跳转。例如，你购买的域名是zackku.com，原来的是zack-ku.github.io。那么你访问前者后，就会跳转到后者，在url地址栏上显示的是后者。这样看起来会非常的low，也没发挥出自买域名的效果。所以，你在你的jekyll项目中还要做一件事情。那就是在项目的根目录下建立一个名字为 CNAME 的文件，注意，一定要大写。然后在文件中写上你域名名字就可以啦。 然后push代码，等待一切生效吧。一般用dnspod解析的话，10分钟左右就会生效，其他据说要48小时内。]]></content>
      <categories>
        <category>工具分享</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>jekyll</tag>
        <tag>域名</tag>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于Jekyll和github Pages博客搭建全过程(一)]]></title>
    <url>%2Fbuilt-blog%2F</url>
    <content type="text"><![CDATA[基于jekyll和github pages博客搭建全过程(二) 基于jekyll和github pages博客搭建全过程(三) 为什么要搭建这样的一个博客 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;一直以来，所有信息的记录几乎都是用印象笔记的，包括工作的note、日常随手、一些学习记录。这样导致一个问题，没有一个很好的地方让自己去写一些总结，自己的心得。不是说印象不可以做，只是那里没有所谓的组织感，大部分都是一些摘录，copy。写blog，维护blog让自己更容易有成就感，因为它是公开的，别人可以通过blog了解你更多的信息，而因为如此你更加有动力去更新更好的文章，弄更好看的界面。其次，写博客是一种很好的消遣，让你学习或工作累了，写写文章，对之前某些事或知识点做回顾和总结。这样比浪费时间好太多。长期下去，积累了许多总结过的精华，当你回顾的时候，你会发现,这，是你一生的财产。 搭建条件像搭建这样的一个基于github pages和jekyll的博客，并不是任何人都可以轻松完成的。至少具备以下条件(可以通过简单学习达到): git的基本使用，用于代码的上传下载 github的基本使用，用于部署静态页面的blog Markdown的基本语法，用于编写格式可控的博客 如果想要更加个性高效的维护开发，需要以下条件(可选): 简单前端知识，html，css等 jekyll的本地安装，用于本地运行博客 域名购买 DNS配置 主要原理github有个github pages功能，把代码的静态页面发布到username.github.io上,username是你github的用户名，像本博客之前的域名就是zack-ku.github.io。其实github pages就是一个jekyll的服务，我们的代码要基于jekyll，才能成功发布。 搭建jekyll博客这个环节对于刚接触的人来说感觉好像很难，其实我们并不用真正完整去学习jekyll才能搭建blog，我们主要的目的是写blog。 最简单的方法github pages的主页 里面就介绍了最简单最快速，小白都会的搭建个人博客的方法(里面连git的命令都一步步告诉你)，就是用github项目设置中的Automatic page generator。它会自动把你的代码清空掉，重构成基于jekyll的项目，并且你可以自由选择博客的主题风格，连一点代码都不需要写。 定制的方法http://jekyllthemes.io http://jekyllthemes.org上面有直接下载的jekyll模板，不过每个模板的功能都不一样，你要想很好的使用它，你需要一点代码的功底和前端的知识才能比较轻松的定制出自己的blog。另外你可以上github去搜索多人关注的jekyll项目，fork一个。 博客编写一般jekyll的文件目录结构如下虽然不同模板可能不尽相同，但是一定会有 _posts 这个文件夹，而这个文件夹正是编写博客的地方，如果不关注网站的其他方面，那只管在这文件夹里面写博客就好了。在这个文件夹里面新增一个.md格式的文件，文件名格式一般需要以日期开头，标题结尾，如&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2016-07-09-how-to-build-a-blog.md注意文件名不能有空格，否则在用url读取时会出现问题。至于博客具体编写是需要用到Markdown的。这语言非常容易学，不用5分钟就可以学会使用了。具体的基础语法可见本博客 Markdown基础语法 到这为止，你已经拥有一个专属的个人博客，并且可以在上面自由的撰写美文了。具体的关于jekyll的深入，和域名、DNS配置，请留意后续博文。 参考 : https://pages.github.com github pages主页 http://jekyllcn.com/ jekyll中文主页]]></content>
      <categories>
        <category>工具分享</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>jekyll</tag>
        <tag>github pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown基础语法]]></title>
    <url>%2FMarkdown%2F</url>
    <content type="text"><![CDATA[这是我博客的第一篇文章。因为第一次使用Markdown，先写本文用于学习与总结，作为博客之路的开篇。网上查看了好多关于Markdown的资料，大多都是零散或者没有关注重点。本篇总结就是为了写博文而生，当本文完成之时，即掌握Markdown之时。而且当以后写博客忘记了语法，可以点击这篇文章，方便的查阅。 换行Markdown中，换行并非一个回车就能换行，如果想换行，需要输入 空格+空格+回车 刚刚输入了，来到这里，否则换行输入满屏才能自动换行的。实际上只要输入多于两个空格再按回车都可以换行。 空格缩进Markdown中，直接输入多个空格，多余不会解释出来，就像html那样。 半方大的空白 &amp;ensp; 或 &amp;#8194; 全方大的空白 &amp;emsp; 或 &amp;#8195; 不断行的空白格 &amp;nbsp; 或 &amp;#160; 一般用 &amp;nbsp;就行了，比较容易记 牛逼superman 前后加 &amp; 和 ； 效&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;果&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;如&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;此&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;！ 标题标题是每篇文章都需要也是最常用的格式，在 Markdown 中，如果一段文字被定义为标题，只要在这段文字前加 # 号即可。 # 一级标题 ## 二级标题 ### 三级标题 以此类推 # 越多，标题越小。效果如下： 一级标题二级标题三级标题 文本高亮用` （键盘左上角数字1旁边的按键）包起来的文字就会高亮，虽然说是高亮，其实inspect元素看看，html的标签和代码框一样是code，只是代码框多了css的设置而已，所以说这个高亮才是最原始的code，只是不同的页面对此设置的css不一样，所以有时会高亮，有时是短代码。 `文本` 效果 文本 引用当编写文章时需要引用到别人的话语时，可以用在文字前加上&gt;符号来引用 &gt;引用 &gt;一段话 引用 一段话 并且在引用中可以输入多个 &gt; 符号作为多重引用 而且在引用中，同时还可以用其他的markdown语法 引用Hello World代码 列表数字列表1. 一 2. 二 3. 三 注意 1. 后面是有空格的，否则不生效 一 二 三 无序列表- 一 - 二 - 三 同样 - 后面是有空格的 一 二 三 强调*single asterisks* _single underscores_ **double asterisks** __double underscores__ single asterisks 引用强调 single underscores double asterisks 文本强调 double underscores 代码框连续4个空格以上（等长以上tab） 然后输入文字，文字就会在代码框中 function log(str) { console.log(str) } 图片![markdown tips](/assets/photo/markdown-title.png) []中填写图片的tips,()填写图片路径 链接[an example](https://zack-ku.github.io/ &quot;Title&quot;) 指向link有title [This link](https://zack-ku.github.io/) 无title &lt;https://zack-ku.github.io//&gt; 自动链接 效果： https://zack-ku.github.io// 参考 : http://wowubuntu.com/markdown Markdown 语法说明 (简体中文版)]]></content>
      <categories>
        <category>其他技术</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
</search>
